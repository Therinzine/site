<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.101.0"><link rel=canonical type=text/html href=purduearc.com/wiki/active-projects/robot-arm/><link rel=alternate type=application/rss+xml href=purduearc.com/wiki/active-projects/robot-arm/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/purduearc.com/favicons/favicon.ico><link rel=apple-touch-icon href=/purduearc.com/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/purduearc.com/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/purduearc.com/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/purduearc.com/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/purduearc.com/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/purduearc.com/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/purduearc.com/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/purduearc.com/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/purduearc.com/favicons/android-192x192.png sizes=192x192><title>Robot Arm | Purdue ARC</title><meta name=description content="Building a open-source, 3d-printed manipulator
"><meta property="og:title" content="Robot Arm"><meta property="og:description" content="Building a open-source, 3d-printed manipulator
"><meta property="og:type" content="website"><meta property="og:url" content="purduearc.com/wiki/active-projects/robot-arm/"><meta property="og:site_name" content="Purdue ARC"><meta itemprop=name content="Robot Arm"><meta itemprop=description content="Building a open-source, 3d-printed manipulator
"><meta name=twitter:card content="summary"><meta name=twitter:title content="Robot Arm"><meta name=twitter:description content="Building a open-source, 3d-printed manipulator
"><link rel=preload href=/purduearc.com/scss/main.min.505db32259e554af32fbaefe47c719d4950cff2f8432a0552a1c0e45ca7f147a.css as=style><link href=/purduearc.com/scss/main.min.505db32259e554af32fbaefe47c719d4950cff2f8432a0552a1c0e45ca7f147a.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-00000000-0","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=purduearc.com/><span class=navbar-logo></span><span class=font-weight-bold>Purdue ARC</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=purduearc.com/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=purduearc.com/wiki/><span class=active>Wiki</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=purduearc.com/blog/><span>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=purduearc.com/join/><span>Join</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=purduearc.com/wiki/active-projects/robot-arm/>Return to the regular view of this page</a>.</p></div><h1 class=title>Robot Arm</h1><div class=lead>Building a open-source, 3d-printed manipulator</div><ul><li>1: <a href=#pg-3cf4f3bfb079be3d11ea57b6fedc725b>Software</a></li><li>2: <a href=#pg-bbd7a74090357072550611c9bd860e2e>Hardware</a></li><li>3: <a href=#pg-5a81c8e00f9a700ac0041333f54dc646></a></li><ul></ul></ul><div class=content><h2 id=goal>Goal</h2><h3 id=mission>Mission</h3><p>Our overarching goal is to explore why robots are limited to factory environments and why we don&rsquo;t have them cooking for us and folding our clothes yet.</p><p>In our journey, we plan to publish our progress, tutorials, code, and workshops.</p><h3 id=right-now>Right now</h3><p>Right now, we&rsquo;re building software/hardware for a robot arm to play chess.</p><p>From this, we hope to validate fundamental understanding of robot arm design, control (using control systems and/or reinforcement learning), sensor systems (vision, encoders, tactile), and mechanical actuator systems (servos, steppers, gearboxes).</p><h4 id=hardware>Hardware</h4><p>We currently have two hardware efforts:</p><ol><li>Designing a <a href=https://wiki.purduearc.com/wiki/robot-arm/hardware#mr-janktastic>Mr. Janktastic</a> from scratch</li><li>Building the <a href=https://wiki.purduearc.com/wiki/robot-arm/hardware#bcn3d-moveo-arm>open-source BCN3D Moveo Arm</a> for a better hardware system for software to experiment and test vision, RL systems with.</li></ol><h4 id=software>Software</h4><p>Working on a variety of problems in vision, control, and high level planning (RL soon!)). See the <a href=https://wiki.purduearc.com/wiki/robot-arm/software>Software Docs</a> for a deeper dive.</p><h2 id=what-have-we-done>What have we done?</h2><h3 id=may-2021>May 2021</h3><ul><li>Object detection working on chess pieces with 90%+ accuracy using YOLOv5 and usable in ROS<ul><li>Put together a 500+ chess piece dataset for detection</li></ul></li></ul><img src=images/obj_det_may_21.png alt="Object detection demo" width=400><ul><li>Prototype gripper fingers that can pick up chess pieces decently well</li></ul><p>{% include googleDrivePlayer.html id=&ldquo;1P8rwWDJa1Yuv88X697RMvEq04j1IgpqW/preview&rdquo; %}</p><ul><li>Created Gazebo simulation that is controlled by MoveIt pipeline, including a simulated camera, chessboard, and chess pieces</li></ul><p>{% include googleDrivePlayer.html id=&ldquo;19FZ7lsqCn6DEChjjdBgsTy0feUANYaVO/preview&rdquo; %}</p><h3 id=december-2020>December 2020</h3><ul><li>Got an early version of protoarm to stack some boxes using IK and trajectory planning from MoveIt and ROS</li></ul><p>{% include googleDrivePlayer.html id=&ldquo;1yms3OuqYp-n4JCt-yBGZg8yEyajXOj_M/preview&rdquo; %}</p><ul><li>Designed a prototype 6dof arm</li></ul><img src=images/6dof_dec_20.png alt="6DOF arm CAD" width=200><h2 id=quick-links>Quick links</h2><ul><li><a href=https://github.com/purdue-arc/arc_robot_arm>GitHub</a></li></ul></div></div><div class=td-content style=page-break-before:always><h1 id=pg-3cf4f3bfb079be3d11ea57b6fedc725b>1 - Software</h1><div class=lead>Learn about the robot arm software stack</div><blockquote><p>If you want to look at code and run some of it, check out <a href=https://github.com/purdue-arc/arc_robot_arm>arc_robot_arm</a> README for quick start details and usage instructions.</p></blockquote><h2 id=how-to-use-this-if-youre-new>How to use this if you&rsquo;re new</h2><p>From the diagram and subsection descriptions, you can hopefully get a preliminary high-level understanding of each of the parts and how they fit together.</p><p>From this, you can then:</p><ol><li>Choose which subsection interests you most</li><li>Follow links in the specified subsection to get more contextual understanding (Optionally ask questions in the Discord, hop in a voice chat, etc for more fun convo about the area)</li><li>Ensure you understand the related topics for each subsections with practical understanding/experience by completing a step-by-step tutorial (either use google to find one or ask for suggestions in Discord), ensuring that it uses the related topics (<strong>Can be done concurrently with #2</strong>)</li><li>At this point, you should have enough knowledge/experience in your subsection to contribute to the current approach and possibly even change what the current approach is if you find better ways to do things (if so, update these docs!)</li></ol><h2 id=system-overview>System Overview</h2><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph
  subgraph Behavior Planning 
    BP1[Behavior Planner]
    BP2[Chess Engine or Human]
    %% BP3[RL Policy]
  end
  subgraph Low Level
    direction LR
    LL1[Protoarm ROS Controller]
    LL2[driver]
    LL3[servos] 
  end
  subgraph Simulation
    direction LR
    S1[Gazebo ROS Controller]
    S2[Simulated robot joints from URDF] 
  end
  subgraph Vision 
    direction LR
    V1[YOLOv5 Object Detector ROS node]
    V2[Chessboard Detector] 
  end
  subgraph Planning/Control
    PC1[Visual Servoing ROS node] 
    PC2[MoveTo C++ ROS node]
    PC3[MoveIt] 
  end

  %% Low level
  LL2 -- servo position PWM --&gt; LL3
  LL3 -- potentiometer feedback \- not done --&gt; LL2
  LL1 --&gt; LL2

  %% Behavior Planning
  BP2 --&gt; BP1 
  BP1 --&gt; PC1 

  %% Vision 
  V2 --&gt; BP1 
  V1 --&gt; PC1

  %% Simulation 
  S1 --&gt; S2
  
  %% Planning/Control 
  PC1 --&gt; PC2
  PC2 --&gt; PC3
  PC3 --&gt; S1
  PC3 --&gt; LL1

  %% Links
  click LL1 href &#34;http://wiki.purduearc.com/wiki/robot-arm/software#controller&#34; &#34;Sensors&#34;
  click LL2 href &#34;http://wiki.purduearc.com/wiki/robot-arm/software#driver&#34; &#34;Sensors&#34;

  click PC1 href &#34;http://wiki.purduearc.com/wiki/robot-arm/software#visual-servoing&#34; &#34;Sensors&#34;
  click PC2 href &#34;http://wiki.purduearc.com/wiki/robot-arm/software#kinematics-and-planning&#34; &#34;Sensors&#34;
  click PC3 &#34;https://moveit.ros.org/assets/images/diagrams/moveit_pipeline.png&#34; &#34;Open this in a new tab&#34; _blank

  click BP1 href &#34;http://wiki.purduearc.com/wiki/robot-arm/software#behavior-planner&#34; &#34;Sensors&#34;

  click S1 href &#34;http://wiki.purduearc.com/wiki/robot-arm/software#simulation&#34; &#34;Sensors&#34;
  click S2 href &#34;http://wiki.purduearc.com/wiki/robot-arm/software#simulation&#34; &#34;Sensors&#34;

  click V1 href &#34;http://wiki.purduearc.com/wiki/robot-arm/software#chess-piece-detection&#34; &#34;Sensors&#34;
  click V2 href &#34;http://wiki.purduearc.com/wiki/robot-arm/software#chessboard-detection&#34; &#34;Sensors&#34;

  classDef not_started fill:#ff8181
  classDef in_progress fill:#ffba82
  classDef done fill:#81ff9b
  class BP1,LL1,BP2,BP3,V2 not_started
  class PC1 in_progress
  class S1,S2,LL2,LL3,V1,PC2,PC3 done
</code></pre><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph
  l1[Not Started]
  l2[In Progress]
  l3[Done]
classDef not_started fill:#ff8181
classDef in_progress fill:#ffba82
classDef done fill:#81ff9b
class l1 not_started
class l2 in_progress
class l3 done
</code></pre><h2 id=high-level>High level</h2><h3 id=behavior-planner>Behavior Planner</h3><p>After the robot turns on or at any given point of time, what should the robot do? This is the job of the behavior planner.</p><p>Overall, it should output executable commands that return true or false if they are completed successfully and then output the next command. A preliminary decision flowchart that a robot can make is modeled here:</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph
  a1[Scanning for change in board state]
  p1[Virtual Human]
  p2[Engine]
  a3[Identify and pick up piece]
  a4[Identify destination and place piece]
  a1 -- 2d picture of board --&gt; p1
  a1 -- FEN Notation of board--&gt; p2
  p1 -- Next move --&gt; a3
  p2 -- Next move --&gt; a3
  a3 --&gt; a4 --&gt; a1
</code></pre><p>To implement the behavior planner, a Finite State Machine (FSM) and/or a Behavior Tree (BT) can be used, which both have tradeoffs in <strong>modularity</strong> and <strong>reactivity</strong>, (<a href=https://roboticseabass.com/2021/05/08/introduction-to-behavior-trees/>read more</a>, scroll to the last section for ).</p><p><strong>Related Topics:</strong></p><ul><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#ros>ROS</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#pythonnumpy>Python</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#fsm>FSM</a> and/or <a href=https://wiki.purduearc.com/wiki/robot-arm/software#behavior-trees>BT</a></li></ul><h2 id=vision>Vision</h2><h3 id=chess-piece-detection>Chess Piece Detection</h3><p>How does our robot determine which piece is which and where is that piece relative to the arm? This is a common scenario in many real-world use cases and object detection, as the name suggests, is used to detect the chess pieces.</p><p>The object detection stack consists of the <a href=https://github.com/ultralytics/yolov5>YOLOv5</a> object detection model trained on a custom dataset and outputs 2d bounding boxes. This can then be extrapolated to 3D coordinates from the camera intrinsics relative to the arm with some coordinate transformations from camera -> robot arm base.</p><p>For chess, the model is trained with a custom large chess piece dataset of 500+ images stored on <a href=https://app.roboflow.com/>roboflow</a>. <a href="https://colab.research.google.com/drive/1XJ82eVA0cEfTczXFMtV1sunqvsfiJWQ0?usp=sharing">Here</a> is a simple colab that walks through the training process using a Roboflow dataset.</p><p>Inference is using the <code>yolov5_pytorch_ros</code> ROS package using the <code>detector</code> ROS node using Python/PyTorch, allowing it to communicate detections and classes to other nodes such as for visual servoing. It reaches around 15-20 FPS without a GPU on a Mac.</p><blockquote><p>See <a href=https://github.com/purdue-arc/arc_robot_arm/tree/main/chess_piece_detector>chess_piece_detector</a> for quick start details and usage instructions.</p></blockquote><p><strong>Related Topics:</strong></p><ul><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#ros>ROS</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#pythonnumpy>Python</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#object-detection>Object Detection using YOLOv5</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#coordinate-transforms>Coordinate Transformations</a></li></ul><h3 id=chessboard-detection>Chessboard Detection</h3><p>Playing chess is more than just picking and placing pieces. The robot needs to actually beat the human. We need to know the exact state of the chessboard, so another person or an overpowered engine can say what move to play next.</p><p>We can do this using a computer vision techniques with an image of the current chessboard as an input and the FEN notation of to board as an output. The following diagram shows this visually:</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph LR
  a1[Real chessboard] --&gt; a2[2D chessboard] 
  a2 --&gt; a3[FEN Notation] 
  a3 --&gt; a4[Chess Engine]
  a2 --&gt; a5[Virtual human player]
</code></pre><p>Some external projects we plan to use to complete the above:</p><ul><li><a href=https://github.com/maciejczyzewski/neural-chessboard>Real chessboard -> 2D chessboard</a></li><li><a href=https://github.com/Elucidation/tensorflow_chessbot>2D chessboard -> FEN Notation</a></li></ul><p><strong>Related Topics:</strong></p><ul><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#ros>ROS</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#pythonnumpy>Python</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#machine-learning>Machine Learning</a></li></ul><h2 id=control>Control</h2><h3 id=visual-servoing>Visual Servoing</h3><p>Even if we can see where the pieces are with the camera, how does the robot arm move closer to the chess piece it wants to pick up? Without visual servoing, any error that the robot makes cannot be accounted for and adjusted for accordingly, resulting in lots of fails. Visual servoing is a control algorithm using images as input to control for any errors that the robot makes.</p><p>We are using image-based visual servoing to localize the robot arm hand over an object in a graspable configuration, using images/2d bounding boxes as inputs to the system and servo commands as outputs.</p><img src=/wiki/active-projects/robot-arm/images/vs-diagram.png alt="Diagram of controller" width=400>
> Visual servoing system diagram ([source](https://www.researchgate.net/figure/Visual-servoing-closed-loop_fig1_265436696))<p>See the <code>protoarm_visual_servoing</code> package for more details.</p><p><strong>Related Topics:</strong></p><ul><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#ros>ROS</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#pythonnumpy>Python</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#visual-servoing-1>Visual Servoing</a></li><li>Usage of the MoveTo node</li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#coordinate-transforms>Coordinate Transforms</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#robot-arm-kinematics>Robot Arm Kinematics</a></li></ul><h3 id=kinematics-and-planning>Kinematics and Planning</h3><p>As a human, it is simple to move our joints and pick something up in 3D space. For a robot arm, it knows nothing of 3D space, only numerical angles for each of its joints. So, we use <a href=https://en.wikipedia.org/wiki/Inverse_kinematics>inverse kinematics</a> (IK), the mathematical process of converting 3D space coordinates to joint angles, to determine the final position for the robot arm.</p><p>Now what happens between the start and final position? That&rsquo;s the job of the motion planner. It determines a safe collision-free trajectory for each joint and creates the plan. Then, the plan is executed, either in real life or in simulation.</p><p>As of now, all the kinematics and planning heavy lifting is done by <a href=http://moveit.ros.org/>MoveIt</a>. The <code>protoarm_kinematics</code> package houses a wrapper written in C++ that abstracts the process of sending <a href=http://docs.ros.org/en/melodic/api/sensor_msgs/html/msg/JointState.html>JointState</a> or <a href=http://docs.ros.org/en/noetic/api/geometry_msgs/html/msg/Pose.html>Pose</a> goals.</p><p>The wrapper is interfaced externally using the <code>move_to</code> node. Refer to the <a href=https://github.com/purdue-arc/arc_robot_arm/blob/main/protoarm_kinematics/src/test_kinematics>test_kinematics</a> rospy file in <code>protoarm_kinematics/src</code> for usage of the <code>move_to</code> node.</p><p>This package is also where we would keep our custom kinematics plugin and planning library if we choose to make it from scratch, instead of the default <a href=https://ros-planning.github.io/moveit_tutorials/doc/kinematics_configuration/kinematics_configuration_tutorial.html#the-kdl-kinematics-plugin>KDL</a> kinematics plugin and <a href=https://ros-planning.github.io/moveit_tutorials/doc/ompl_interface/ompl_interface_tutorial.html>OMPL</a> motion planning library.</p><blockquote><p>See <a href=https://github.com/purdue-arc/arc_robot_arm/tree/main/protoarm_kinematics>protoarm_kinematics</a> for quick start details and usage instructions.</p></blockquote><p><strong>Related Topics:</strong></p><ul><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#ros>ROS</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#pythonnumpy>Python</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#moveit>MoveIt</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#coordinate-transforms>Coordinate Transforms</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#robot-arm-kinematics>Robot Arm Kinematics</a></li></ul><h2 id=low-level>Low level</h2><h3 id=controller>Controller</h3><p><code>protoarm_control</code> is the ROS package that ensures that MoveIt execution commands are executed exactly as expected and to a degree of certainty in the real world.</p><p>Right now, the driver communicates directly with MoveIt as the <code>protoarm_control</code> package doesn&rsquo;t exist yet and because we do not have servo feedback.</p><p>Given that the protoarm uses some of the cheapest servos on the market, how can we still get dependable sub-millimeter precision to do tasks reliably? One way to do so is to hack our servos to add encoders, and use a control scheme that can take velocity and torque into account.</p><p>Inspired by Adam&rsquo;s <a href=https://github.com/adamb314/ServoProject>Servo Project</a>.</p><p><strong>Related Topics:</strong></p><ul><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#ros>ROS</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#pythonnumpy>Python</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#cascade-style-control>Cascade-style Control</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#encoders>Encoders</a></li></ul><h3 id=driver>Driver</h3><p>The <code>protoarm_driver</code> is written in Arduino that actually interfaces with the servos and encoders. It sets joint limits, does coordinate frame conversions from the URDF to the actual robot, converts MoveIt angles to servo joint angles $$(-\pi,\pi)$$ to $$(0^\circ,180^\circ)$$, and executes servo commands using PWM to the servos.</p><p><strong>Related Topics:</strong></p><ul><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#arduino>Arduino</a></li></ul><h2 id=simulation>Simulation</h2><h3 id=gazebo>Gazebo</h3><p>There are lots of benefits from simulation spanning from speeding up development and testing of software, realistic environments for reinforcement learning, and testing proof of concepts.</p><p>Our arm is simulated in Gazebo with a Realsense D435 camera (<code>realsense_ros_gazebo</code>), chessboard, and chess pieces (<code>chessboard_gazebo</code>). The robot arm and the camera are represented in <a href=http://wiki.ros.org/urdf/XML/model>URDF</a>.</p><p>Sensors, actuation, gazebo plugins, and more are <a href=http://wiki.ros.org/urdf/XML>specifications</a> that can be added. For the arm, these specifications exist in the <code>.xacro</code> files in the <code>urdf</code> folder of the <code>protoarm_description</code> ROS package.</p><p>Gazebo uses SDF models (in <code>models</code> folder of <code>chessboard_gazebo</code>) to represent static assets in the simulation like the chess pieces and chessboard. These objects are then spawned into a Gazebo world (along with the robot URDF), represented with a <code>.world</code> file.</p><blockquote><p>See <a href=https://github.com/purdue-arc/arc_robot_arm/tree/main/protoarm_bringup>protoarm_bringup</a> for quick start details and usage instructions.</p></blockquote><p>{% include googleDrivePlayer.html id=&ldquo;19FZ7lsqCn6DEChjjdBgsTy0feUANYaVO/preview&rdquo; %}</p><p><strong>Related Topics:</strong></p><ul><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#gazebo-1>Gazebo</a></li><li><a href>URDF</a></li></ul><h1 id=resources>Resources</h1><h2 id=tools>Tools</h2><h3 id=ros>ROS</h3><ul><li><a href=https://wiki.purduearc.com/wiki/tutorials/snake-tutorial>ARC ROS tutorials (Recommended)</a></li><li><a href=http://wiki.ros.org/ROS/Tutorials>Official ROS tutorials</a></li></ul><h3 id=github>GitHub</h3><ul><li><a href=https://www.freecodecamp.org/news/learn-the-basics-of-git-in-under-10-minutes-da548267cc91/>Learn Git under 10 min</a></li></ul><h3 id=c>C++</h3><ul><li><a href="https://www.youtube.com/watch?v=18c3MTX0PK0&list=PLlrATfBNZ98dudnM48yfGUldqGD0S4FFb">In depth tutorial playlist for C++</a><ul><li>Recommended topics:<ul><li>if/else, loops, functions, classes</li><li>Pointers/References</li><li>Smart pointers</li><li>Dynamic Arrays (std::vector)</li></ul></li></ul></li><li>Very useful numeric libraries<ul><li><a href="https://eigen.tuxfamily.org/index.php?title=Main_Page">Eigen</a>: Extremely efficient matrix math library</li></ul></li></ul><h3 id=pythonnumpy>Python/Numpy</h3><p><strong>Python</strong></p><ul><li><a href="https://www.youtube.com/watch?v=rfscVS0vtbw">Tutorial</a></li><li>Important topics to understand:<ul><li>Basics are good - variables + logic, functions, classes</li></ul></li></ul><p><strong>Numpy</strong></p><blockquote><p>Must use when working with large arrays (i.e images)</p></blockquote><ul><li><a href="https://www.youtube.com/watch?v=8Y0qQEh7dJg">Tutorial</a></li><li>Important topics to understand:<ul><li>Creating arrays</li><li>slicing + indexing</li><li>reshaping</li><li>linear algebra</li></ul></li></ul><p><strong>OpenCV</strong>:</p><blockquote><p>Use for computer vision and image transformations like color detection, tracking, etc</p></blockquote><ul><li><a href="https://www.youtube.com/watch?v=oXlwWbU8l2o">Tutorial</a></li><li>Important topics to understand:<ul><li>image transformation</li><li>thresholding</li><li>read/write images from file</li><li>resizing</li><li>tracking</li></ul></li></ul><h3 id=arduino>Arduino</h3><ul><li>Top hit for &ldquo;arduino tutorial&rdquo; on google should work</li></ul><h3 id=gazebo-1>Gazebo</h3><ul><li>Important topics (Googlable)<ul><li>Creating a world</li><li>Adding assets</li><li>ROS control in Gazebo</li><li>Cameras in Gazebo</li></ul></li></ul><h3 id=good-coding-practices>Good coding practices</h3><ul><li><a href=https://cpp-optimizations.netlify.app/>Optimized C++ code</a></li></ul><p>TODO</p><h2 id=algorithms-theory-math>Algorithms, Theory, Math</h2><h3 id=coordinate-transforms>Coordinate Transforms</h3><ul><li><a href=http://faculty.salina.k-state.edu/tim/robotics_sg/Pose/Pose.html>Understanding the pose of a robot</a></li></ul><h3 id=fsms>FSMs</h3><ul><li><a href=https://www.clear.rice.edu/engi128/Handouts/Lec17-Robotics.pdf>tutorial slideshow</a></li></ul><h3 id=behavior-trees>Behavior Trees</h3><ul><li><a href=https://roboticseabass.com/2021/05/08/introduction-to-behavior-trees/>intro to behavior trees</a></li></ul><h3 id=control-algorithms>Control Algorithms</h3><h4 id=pid-controller>PID Controller</h4><ul><li><a href="https://ctms.engin.umich.edu/CTMS/index.php?example=Introduction&section=ControlPID">tutorial</a></li></ul><h4 id=cascade-controller>Cascade Controller</h4><ul><li><a href=https://github.com/adamb314/ServoProject>Implememtation by Adam for Servo Project</a></li></ul><h3 id=ai>AI</h3><h4 id=object-detection>Object Detection</h4><ul><li><a href=https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/>train yolov5 model on custom dataset</a></li></ul><h4 id=machine-learning>Machine Learning</h4><h2 id=sensor-related>Sensor related</h2><h3 id=camera>Camera</h3><ul><li><a href>Camera driver ROS package</a></li><li><a href>Camera calibration ROS package</a></li></ul><h3 id=encoders>Encoders</h3><ul><li><a href=https://www.arduino.cc/reference/en/libraries/encoder/>Arduino Encoder Package</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-bbd7a74090357072550611c9bd860e2e>2 - Hardware</h1><div class=lead>Learn about the robot arm hardware</div><h2 id=meet-the-robots>Meet the robots</h2><h3 id=protoarm>Protoarm</h3><p>This is protoarm (short for prototype arm), a 5-DOF robot arm adapted slightly from <a href="https://www.youtube.com/watch?v=_B3gWd3A_SI">HowToMechatronics&rsquo; model</a> that we built first to understand ROS, MoveIt, and the software stack for robot arms.</p><img src=/wiki/active-projects/robot-arm/images/protoarm.png alt="Protoarm CAD" width=400><h3 id=bcn3d-moveo-arm>BCN3D Moveo Arm</h3><p>This is the <a href=https://www.bcn3d.com/bcn3d-moveo-the-future-of-learning-robotic-arm/>BCN3D Moveo</a> arm that is fully open-source, built with steppers rather than cheap servos which have feedback control (a step up from the Protoarm), and is actually big enough to reach a full size chessboard. It also is much better designed and assembled than the Protoarm.</p><p>Quick links:</p><ul><li><a href=https://github.com/BradyHooverDesigns/BCN3D-MOVEO-BOM>BOM</a></li><li><a href=https://github.com/BCN3D/BCN3D-Moveo>CAD/Instructions</a></li></ul><img src=/wiki/active-projects/robot-arm/images/moveo.jpg alt=Moveo width=400><h3 id=mr-janktastic>Mr. Janktastic</h3><p>Building a 6-DOF robot arm fully from scratch with Dynamixel Servos.</p><img src=/wiki/active-projects/robot-arm/images/janktastic.png alt=Moveo width=400><h2 id=resources>Resources</h2><h3 id=cad>CAD</h3><p>TODO</p><h3 id=manufacturing>Manufacturing</h3><h4 id=3d-printing>3D Printing</h4><p>TODO</p><h4 id=metal-manufacturing-tools>Metal Manufacturing tools</h4><p>TODO</p><h4 id=laser-cutting>Laser Cutting</h4><p>TODO</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5a81c8e00f9a700ac0041333f54dc646>3 -</h1></div></main></div></div><footer class="bg-dark py-5 row d-print-none"><div class="container-fluid mx-sm-5"><div class=row><div class="col-6 col-sm-4 text-xs-center order-sm-2"></div><div class="col-6 col-sm-4 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank rel=noopener href=https://mobile.twitter.com/purduerobotics aria-label=Twitter><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank rel=noopener href=https://github.com/purdue-arc aria-label=GitHub><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Discord aria-label=Discord><a class=text-white target=_blank rel=noopener href=https://discord.gg/ddkzfD2cyu aria-label=Discord><i class="fab fa-discord"></i></a></li></ul></div><div class="col-12 col-sm-4 text-center py-2 order-sm-2"><small class=text-white>&copy; 2022 Purdue ARC All Rights Reserved</small>
<small class=ml-1><a href=https://policies.google.com/privacy target=_blank rel=noopener>Privacy Policy</a></small></div></div></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/purduearc.com/js/main.min.5909da3f4d19977acc73621d687cb3539ee462949171a6c447bb67600aa79953.js integrity="sha256-WQnaP00Zl3rMc2IdaHyzU57kYpSRcabER7tnYAqnmVM=" crossorigin=anonymous></script></body></html>