<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.111.3"><link rel=canonical type=text/html href=https://purduearc.com/wiki/active-projects/><link rel=alternate type=application/rss+xml href=https://purduearc.com/wiki/active-projects/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Active Projects | Purdue ARC</title><meta name=description content="The Autonomous Robotics Club at Purdue"><meta property="og:title" content="Active Projects"><meta property="og:description" content="The Autonomous Robotics Club at Purdue"><meta property="og:type" content="website"><meta property="og:url" content="https://purduearc.com/wiki/active-projects/"><meta property="og:site_name" content="Purdue ARC"><meta itemprop=name content="Active Projects"><meta itemprop=description content="The Autonomous Robotics Club at Purdue"><meta name=twitter:card content="summary"><meta name=twitter:title content="Active Projects"><meta name=twitter:description content="The Autonomous Robotics Club at Purdue"><link rel=preload href=/scss/main.min.5f4076087349a426baccf15da1bf1377e3d88edfa841e29d6e8c1eed1083fd39.css as=style><link href=/scss/main.min.5f4076087349a426baccf15da1bf1377e3d88edfa841e29d6e8c1eed1083fd39.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-00000000-0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","UA-00000000-0")}</script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"></span><span class=navbar-brand__name>Purdue ARC</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=/wiki/><span class=active>Wiki</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog/><span>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/join/><span>Join</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><div class=td-search><div class=td-search__icon></div><input type=search class="td-search__input form-control td-search-input" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/wiki/active-projects/>Return to the regular view of this page</a>.</p></div><h1 class=title>Active Projects</h1><ul><li>1: <a href=#pg-0c2fc143e8a2fe04807df603231b1dcf>Rocket League</a></li><ul><li>1.1: <a href=#pg-1ed627e854ef9a1cdbe731ad50f47162></a></li><ul></ul></ul><li>2: <a href=#pg-580a5d47f86fc1d27cd99ee7495ebffb>Piano Hand</a></li><ul><li>2.1: <a href=#pg-4816737a5b70c99f2f788a637b590ade></a></li><ul></ul></ul><li>3: <a href=#pg-142504be7ac35674946b2bad97136296>Drone Delivery</a></li><ul><li>3.1: <a href=#pg-8aac8c831dd69d6d6b31226a44e9fd85>Fall 2022 Progress Updates</a></li><li>3.2: <a href=#pg-81f9cbdbe61c3f4f5fcc1f73b61ccff9>Pre-flight path planning</a></li><li>3.3: <a href=#pg-2eccd07d908e3c28fb3ef2ed368c6290></a></li><ul></ul></ul><li>4: <a href=#pg-979cc4dd70e1b73a24f4283657830a48>Robot Arm</a></li><ul><li>4.1: <a href=#pg-3cf4f3bfb079be3d11ea57b6fedc725b>Software</a></li><li>4.2: <a href=#pg-bbd7a74090357072550611c9bd860e2e>Hardware</a></li><li>4.3: <a href=#pg-5a81c8e00f9a700ac0041333f54dc646></a></li><ul></ul></ul><li>5: <a href=#pg-a6210bbefa7cfc9afc14edbf00e77cf4>Wizard's Chess</a></li><ul><li>5.1: <a href=#pg-443684d680c960d7f4e4cc6230251124>Software</a></li><li>5.2: <a href=#pg-c1fe4798378a51fdf5afb95fe2e6819c>Hardware</a></li><li>5.3: <a href=#pg-f1d1a2f654fdfaef02740bab72b0d54d></a></li><ul></ul></ul></ul><div class=content></div></div><div class=td-content><h1 id=pg-0c2fc143e8a2fe04807df603231b1dcf>1 - Rocket League</h1><div class=lead>Teaching cars to play soccer</div><h2 id=goal>Goal</h2><p>Build a system of autonomous, scaled vehicles to play head-to-head in a game of high-speed soccer. Inspiration draws from the game, Rocket League, in which rocket-powered vehicles play soccer in 3v3 matches.</p><p>Current tasks are aimed at creating an interactive demo, where a team of human controlled cars compete against a team of autonomous cars. Future work may entail using the working system to launch a multi-university competition. Teams would build their own autonomous strategies and face-off in a tournament bracket.</p><h2 id=system-overview>System Overview</h2><p>Our system is organized into several components, which function together to create an autonomous Rocket League car.</p><p><img src=images/system-overview.png alt="System Overview"></p><p>The system consists of several specialized layers, which each reduce abstraction as information flows from the top of the diagram to the bottom. For example, a control effort (ex: put the steering wheel at 10 degrees) is less abstract than a collision goal (ex: hit the ball at position (3,5) cm in the (1,0) direction). Each layer refines the previous command until it is eventually something usable by the car&rsquo;s hardware.</p><p>Each layer also utilizes feedback in order to correct errors. For example, the velocity controller may notice the car&rsquo;s velocity is too fast, and then step slightly off the throttle to correct. Each layer uses a common perception system as a truth to compare against.</p><p>For more information on each layer / subsystem, see the below sections to learn more about its input and output, and how it processes it (including the software and languages in use).</p><h3 id=intercommunication>Intercommunication</h3><p>Like most ARC projects, Rocket League uses <a href=https://www.ros.org/about-ros/>ROS</a> to handle communication between each component. ROS is compatible with the Python, C++, and Arduino used on the project.</p><blockquote><p>Documentation on setting up and learning about your ROS development evironment can be found <a href=/wiki/tutorials/ros>here</a>.</p></blockquote><h3 id=high-level-planner--reinforcement-learning>High Level Planner / Reinforcement Learning</h3><p>This component uses deep reinforcement learning in order to develop strategies for playing Rocket League. It is still being prototyped in Python, using PyTorch and Keras (Tensorflow). When complete, it will recieve the full state of the game, and output a collision goal (where, when, and in what direction to hit the ball) to the Mid Level Software stack.</p><p>Rocket League High-Level Planner Update:
Our team has made significant progress in developing the High-Level Planner for the Rocket League project using deep reinforcement learning to generate game strategies. The current prototype is built in Python using Stable Baselines 3, an OpenAI deep learning library built on PyTorch and Keras (Tensorflow). It will ultimately provide instructions to the car, such as acceleration and steering direction, via radio communication.</p><p>From Fall 2021 till Spring 2023, we accomplished the following:
• Created an initial training script that enables the training of multiple simulators in parallel for efficient performance. It uses a vectorized simulator environment, and creates a Proximal Policy Optimization model with the Stable Baselines 3 library.
• Developed a hyperparameter tuning script that tests different combinations of network hyperparameters to find the most effective ones.
• Created a multi-processing script that enables the simultaneous training of multiple models with different reward and environment variables.
• Successfully trained a model to consistently score goals in one goal, overcoming issues such as riding against walls, excessive turning, and imprecise turns.</p><p>For future work, we plan to explore alternative models to PPO and train one agent to compete against another agent. We will also work on training the agent to perform well with noisy data.</p><p>It utilizes a simulator for training, which is described below.</p><h4 id=simulator>Simulator</h4><p>This component exists for training the High Level Planner, and testing / debugging Mid Level Software. It is written in Python, using the Box2D physics engine, and must realistically simulate all physical elements of the game. It can be used to replace everything below Mid Level Software (including the Velocity Controller and Perception) if the entire game is to be run in simulation.</p><h3 id=mid-level-sotware>Mid Level Sotware</h3><p>Together, the Trajectory Planner and Waypoint Controller (both implemented in Python) recieve a collision goal and are responsible for guiding the car to acheive the goal by outputting instantaneous velocity commands for the car.</p><h4 id=trajectory-planner>Trajectory Planner</h4><p>This component considers the car&rsquo;s current location and velocity, and the collision goal, to generate a trajectory for the car to follow. It can operate in several modes to generate trajectories via different mathematical functions, and has many many many configurable settings.</p><h4 id=waypoint-controller>Waypoint Controller</h4><p>This component is what enables the car to follow the generated trajectories. It commands the car to follow specific velocities and wheel angles by applying the <a href="https://www.mathworks.com/help/robotics/ug/pure-pursuit-controller.html#:~:text=Pure%20pursuit%20is%20a%20path,in%20front%20of%20the%20robot.&amp;text=You%20can%20think%20of%20this,point%20in%20front%20of%20it.">pure pursuit algorithm</a> on the path given by the trajectory planner.</p><h3 id=velocity-controller>Velocity Controller</h3><p>This component (also called the low-level controller) adjusts the control efforts (specific throttle and steering values) such that the velocity and heading of the car matches the desired setpoint from the waypoint controller. It implements a PID controller, and is currently written in Python. Future work may see it ported to a different language, such as C++ or MATLAB.</p><h3 id=hardware-inferface>Hardware Inferface</h3><p>This component allows communication to occur between the ROS network and the RC car.</p><p>Control efforts to the car are broadcasted using a FrSky XJT transmitter. These messages are encoded by an Arduino script running on a Teensy 3.1, which communicates to the radio using a digital PPM signal. ROS Serial is used to send the desired efforts to be encoded to the Teensy from the ROS network.</p><p>An image of the hardware for one car is shown below:</p><p><img src=images/ros-interface.jpg alt="ROS Interface Hardware"></p><h3 id=car>Car</h3><p>The car is the complete physical system of one player on the field. Tests were performed on off-the-shelf cars, however none met the desired criteria for acceleration and control. To solve this issue, the team upgraded the electronics of the best-tested car and found much increased performance.</p><div class=embed-container><iframe width=640 height=480 src=https://drive.google.com/file/d/1hoZkHQMXcIDrOJjSXYIXwCfNXiyw8jH6/preview frameborder=0 allowfullscreen></iframe></div><blockquote><p>Left: upgraded car, middle & right: stock cars</p></blockquote><p>The car&rsquo;s upgrades replaced the servo motors, receiver, speed controller, and battery.</p><blockquote><p>Documentation will be created in the Fall 2021 semester to have step-by-step upgrade instructions to build a matching car</p></blockquote><h3 id=environment>Environment</h3><p>Work has been done towards creating a consistent environment for operating the cars and providing infrastructure for localization.</p><p>In Spring 2021, physical tests were performed on Krach&rsquo;s carpet and used plywood planks to provide boundaries. Tripods with PVC tubes were also used to hold multiple cameras necessary for localization.</p><p><img src=images/full-field.png alt="Full Field"></p><p>Future work intends on using aluminum square tubing to rigidly mount cameras with the addition of 3D printed mounts.</p><p><img src=images/camera-mounting.png alt="Camera mount"></p><h3 id=perception>Perception</h3><p>The perception system is responsible for tracking odometry (position, orientation, linear velocity, and angular velocity) for each car and the position and velocity of the ball.</p><p>In the prototype system, cars are tracked through <a href="https://april.eecs.umich.edu/software/apriltag#:~:text=AprilTag%20is%20a%20visual%20fiducial,tags%20relative%20to%20the%20camera.">AprilTags</a> and the ball through OpenCV color thresholding techniques. ARC uses C++ for both systems.</p><p>In order to capture the size of the operating field, multiple cameras are required. The current system uses two PointGrey (FLIR) cameras and two Basler cameras.</p><p>Processing AprilTags for each camera is computationally expensive, so the team invested in a &ldquo;Computation Cart&rdquo; with two desktop PCs. Each PC is responsible for two cameras.</p><p><img src=images/computation-cart.jpg alt="Computation Cart"></p><p>Information from each desktop is then communicated over the ROS network:</p><p><img src=images/multiple-cameras.png alt=Multi-Cam></p><h2 id=future-work>Future work</h2><p>The team has outlined the following objectives in working towards the overall goal:</p><ul><li>Proving the high-level framework on snake game</li><li>Testing and tuning of simulator for usage in training high-level planner</li><li>Completion of camera mounting infrastructure</li><li>Completion of field manufacturing</li><li>Perception system scaling / redesign</li></ul><h2 id=quick-links>Quick links</h2><ul><li><a href=https://github.com/purdue-arc/rocket_league>GitHub</a></li><li><a href="https://drive.google.com/file/d/1zw7jYFSYIVamnQTyYaT1TCJGP7sZOg1J/view?usp=sharing">Spring 2021 Presentation</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-1ed627e854ef9a1cdbe731ad50f47162>1.1 -</h1></div><div class=td-content style=page-break-before:always><h1 id=pg-580a5d47f86fc1d27cd99ee7495ebffb>2 - Piano Hand</h1><div class=lead>Building a human-like hand capable of playing piano</div><h2 id=goal>Goal</h2><p>The primary objective of Piano Hand is to explore the perspective of robotics in replicating human-motion. Fascination for this included looking at tasks that would help us understand biomechanics, which is an area of robotics that has gained a lot of traction recently.</p><p>The goal of the project is to build a fully autonomous robot arm that can play the piano. The human hand, with 27 degrees of freedom (DOF), has so far been the most dextrous mechanism to play the piano and the closer we get to replicating that degree of freedom and movement, the better it is to move the arm and play the piano.</p><p>This semester (Fall 2022), we are planning to refine our working model of the animatronic hand built last semester with the help of accurate servo motor and flex sensor functioning. We would expect to extend functioning to two hands as well. Additionally, we are starting a new path in software along the lines of machine learning and optical image recognition by building a model that can read sheet music, given an image format.</p><p>Along the way, we will publish our progress, code, tutorials and workshops.</p><p>Project GitHub Repository: <a href=https://github.com/purdue-arc/arc-piano-hand><a href=https://github.com/purdue-arc/arc-piano-hand>https://github.com/purdue-arc/arc-piano-hand</a></a></p><hr><h2 id=what-we-have-done>What we have done</h2><h3 id=summer-2022>Summer 2022</h3><p>Ideated and designed new mechanisms for the hand that involves the usage of linear actuators and bevel gears. This design will be assembled and tested in Fall 2022 as a new iteration from the hand design in Spring 2022.</p><img src=images/summer2022_bevel.png alt=Spring2022_Design width=400>
<img src=images/summer2022_linear.png alt=Spring2022_Design width=400><p>Worked on fixing issues that came about in software in Spring 2022, and in getting ready the design to implement on the hand in Fall 2023. Introduced new course of action alongside software to start with machine learning and model development in Optical Music Recognition.</p><p>Clip from working of servos:</p><div class=embed-container><iframe width=640 height=480 src=https://drive.google.com/file/d/12XIlDrckEdbvJNI8A9Jp3um2TsCtu46J/preview frameborder=0 allowfullscreen></iframe></div><h3 id=spring-2022>Spring 2022</h3><img src=images/spring2022_design.png alt=Spring2022_Design width=400><p>Worked on improving model developed in Fall 2021 by printing and testing. An add-on for attaching the servo motors was developed and the design was 3D-printed.</p><img src=images/spring2022_servo.png alt=Spring2022_ServoAttach width=400><p>Software work primarily included setup of environment on Arudino/Raspberry Pi, along with flex sensors. Issues with the usage of continuous servos were addressed. 8-bit ADCs were also used to improve testing.</p><div class=embed-container><iframe width=640 height=480 src=https://drive.google.com/file/d/1XUN3-VXOXNSEd7ECZbARMhXspTH0es3h/preview frameborder=0 allowfullscreen></iframe></div><h3 id=fall-2021>Fall 2021</h3><p>Worked primarily on developing models and getting an idea of the different parts necessary to 3-D print. Produced the following first iteration of the hand by the end of the semester from hardware.</p><img src=images/fall2021_design.png alt=Fall2021_Design width=400><p>Software primarily worked on simulation and testing, and the following simulation was produced on TinkerCAD (TinkerCAD&rsquo;s electronic component simulator had Arduino testing capabilities and hence was useful for the first stage of testing). TInkerCAD&rsquo;s use-cases for simulation testing were visible from early testing with the software for multiple fingers, using MG90S servos.</p><div class=embed-container><iframe width=640 height=480 src=https://drive.google.com/file/d/1FLBJuV58_8QRsNKVJZqaC88cPaUbHi6O/preview frameborder=0 allowfullscreen></iframe></div><hr><h3 id=subteams-and-roster>Subteams and Roster</h3><h4 id=project-manager>Project Manager</h4><p>Revanth Krishna Senthilkumaran, Computer Engineering</p><h4 id=hardware>Hardware</h4><p>Hardware primarily works on making and refining CAD models with tools available, 3D printing models, assembling, testing and identifying points of improvement in the model and testing functionality.</p><ul><li>Rugved Dikay, Aeronautical and Aerospace Engineering</li><li>Akshay, Electrical Engineering</li><li>Archis Behere, Mechanical Engineering</li></ul><h4 id=software>Software</h4><p>Software primarily works on developing the code and algorithms for the movement of the hand to locations computed, along with setup of the electrical systems. More recent initiatives include model development for optical music recognition and Raspberry Pi conversion from Arduino.</p><ul><li>Dhruv Sujatha, Data Science</li><li>Jacob Aldridge, Computer Science</li><li>Manas Paranjape, Computer Science</li><li>Visuwanaath Selvam, Computer Engineering</li></ul><hr><h2 id=resources>Resources</h2><h3>Inspiration / Other Projects</h3><li>A team of researchers attempted replicating the pressure applied in the grasping mechanism and achieved 17 DOF in a 5-finger hand. The actuators used are the most interesting: McKibben Actuators, which move on the basis of difference in Air Pressure. <a href=https://www.sciencedirect.com/science/article/pii/B9780081005743000102>ScienceDirect article</a></li><li>The ILDA Robot Hand: A 15 DOF, highly tactile robot hand, motion along surface of palm, and stepper motor actuation. Some of its capabilities include crushing cans, delicate grasping, and tactile tasks such as tapping. <a href=https://gizmodo.com/lifelike-robotic-hand-is-a-bit-too-close-to-terminator-1848213189>Overview and Videos of Functioning</a> | <a href=https://www.nature.com/articles/s41467-021-27261-0>Nature article</a></li><li>Soft actuated robots - <a href="https://dash.harvard.edu/bitstream/handle/1/25922120/66841469.pdf;sequence=1">Harvard paper</a> |</li><li>WPI - <a href=https://web.wpi.edu/Pubs/E-project/Available/E-project-042919-161531/unrestricted/Piano_Playing_Robotic_Arm_MQP_-_Final.pdf>MQP</li><li>Allegro Hand - <a href=http://wiki.ros.org/Robots/AllegroHand>ROS Documentation</a> | <a href="https://www.youtube.com/watch?v=WzJJ4c6AqnE">MIT Grad Student's Piano Hand</a></li><li>Robot Nano Hand - <a href=https://youtu.be/uOeS_jklU2Y>YouTube</a> | <a href=https://robotnanohand.com/>Site</a></li><li>InMoov Robot Hand - <a href=https://www.thingiverse.com/thing:17773>Parts and Paper Link</a></li><li>Other similar project resources: <a href=https://create.arduino.cc/projecthub/laurencemlai/diy-glove-controlled-robotic-hand-ff5d63>Arduino Project Hub</a> | <a href=https://www.instructables.com/DIY-Robotic-Hand-Controlled-by-a-Glove-and-Arduino/>Instructables</a></li><li>Video Links/Pages referred: <a href=https://www.youtube.com/c/WillCogley>Will Cogley</a> | Automation Robotics' <a href=https://www.youtube.com/channel/UCd0xLOw6No5IAsq3Y2-b0eA>Clone</a></li><h3>References</h3><li>Piano Keys Research - <a href=https://music.stackexchange.com/questions/53847/what-are-the-dimensions-of-piano-keys-in-inches>Dimensions</a></li><li>Joint Design - <a href="https://www.youtube.com/watch?v=AJ9zVYQw5XU">Ball and Socket</a></li><li>Optical Music Recognition Datasets - <a href=https://apacha.github.io/OMR-Datasets/>Apacha Database List</a> | <a href=https://tuggeluk.github.io/deepscores/>Deepscores</a> | <a href=https://grfia.dlsi.ua.es/primus/>Primus</a></li><li>Raspberry Pi-Arduino Connectivity - <a href=https://www.aranacorp.com/en/communication-between-raspberry-pi-and-arduino-with-i2c/>AranaCorp</a> | <a href=https://maker.pro/raspberry-pi/tutorial/raspberry-pi-4-gpio-pinout>Pi4 GPIO</a></li><li>Parts - <a href=https://cdn-learn.adafruit.com/downloads/pdf/adafruit-4-channel-adc-breakouts.pdf>ADCs</a> | Multi-channel Servo Controller (<a href="https://www.adafruit.com/product/2327?gclid=COGwqIfL99ECFYQDaQodjtQPXQ">16</a>, <a href="https://shop.pimoroni.com/products/servo-2040?variant=39800591679571">18</a>) | <a href="https://www.youtube.com/watch?v=2vAoOYF3m8U">Linear Servo Actuators</a> - <a href=https://www.myminifactory.com/object/3d-print-77542>CAD Files 1</a>, <a href=https://www.thingiverse.com/thing:3170748/files>CAD Files 2</a></li><li>Add-ons: Strings/Thread <a href="https://www.amazon.com/Cotton-String-Cooking-Kitchen-Wrapping/dp/B07KVSVTVV/ref=sr_1_1_sspa?crid=3ANYCTZMK7A2X&keywords=cotton+string+3mm&qid=1649018569&sprefix=cotton+string+3mm%2Caps%2C72&sr=8-1-spons&psc=1&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUFDQTNFRjhSR1IzNEkmZW5jcnlwdGVkSWQ9QTA5MDkwNzczUVJLU1FGRTJYME1RJmVuY3J5cHRlZEFkSWQ9QTAzMzE0NzIyV0w0MTJLOU8yNUVTJndpZGdldE5hbWU9c3BfYXRmJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ==">1 </a>, <a href="https://www.amazon.com/gp/product/B01FICZLU8/ref=ewc_pr_img_2?smid=A27A1UMDQYE0QB&th=1">2</a> | <a href="https://www.amazon.com/Winter-Gloves-Men-Women-Waterproof/dp/B09FPS6HHH/ref=sr_1_7?crid=GDR5H6K8SH8I&keywords=amazon%2Bcloth%2Bgloves&qid=1644791622&sprefix=amazon%2Bcloth%2Bglove%2Caps%2C83&sr=8-7&th=1&psc=1">Gloves</a></li><li>DOF Analysis - <a href=https://www.researchgate.net/publication/264907843_Real-time_hand_tracking_for_rehabilitation_and_character_animation>+Real-time Hand Tracking</a> | <a href=https://support.ptc.com/help/creo/creo_pma/r6.0/usascii/index.html#page/simulate/mech_des/connections/calculating_dof_redund.html>Calculator</a> | <a href=https://www.mecharithm.com/degrees-of-freedom-of-a-robot/>Mecharithm</a> | <a href=https://www.techopedia.com/definition/12702/six-degrees-of-freedom-6dof>Technopedia</a></li><li>Interesting Actuation Methods - <a href=https://www.frontiersin.org/articles/10.3389/frobt.2020.586216/full>Voltage-Controlled Linear Actuation</a></li></div><div class=td-content style=page-break-before:always><h1 id=pg-4816737a5b70c99f2f788a637b590ade>2.1 -</h1></div><div class=td-content style=page-break-before:always><h1 id=pg-142504be7ac35674946b2bad97136296>3 - Drone Delivery</h1><div class=lead>Developing an drone that autonomously delivers packages.</div><h2 id=goal>Goal</h2><p>Build and develop a drone that can be used in the applications of last-mile delivery.</p><h2 id=subteams>Subteams</h2><ol><li>Hardware: Build the physical drone, develop the controls algorithm for a flight controller.</li><li>Obstacle Avoidance: Plan a long term route, detect obstacles, and avoid them.</li><li>Flight Controller: Design and build a simple flight controller using Raspberry Pi PICOs.</li><li>Research: Identify the latest technologies in the field that can be used by the other team.</li><li>Mobile App: Build a mobile app that enables users to interface with the drone.</li></ol><h2 id=technology>Technology</h2><h3 id=drone-hardware>Drone hardware</h3><ul><li>Motors: T-Motor U7 V2.0<ul><li>6 total</li><li>4.55kg Lift / Motor</li><li>Over 27kg of Thrust!</li><li>47.5A Draw at 100% Throttle</li></ul></li><li>Props: Tarot 1855<ul><li>18&rsquo;&rsquo; Diameter</li><li>5.5&rsquo;&rsquo; Pitch</li><li>Carbon fiber</li></ul></li><li>Frame: Tarot T960<ul><li>Hexacopter Configuration</li><li>960mm Diameter</li></ul></li><li>Battery: Tattu Plus LiPo Battery Pack<ul><li>22000mAh</li><li>25C Discharge Rate</li><li>6S</li><li>22.2V</li></ul></li><li>Power Delivery (ESC): xRotor 40A<ul><li>60A Max Current</li><li>Rated for 6S LiPo (22.2V)</li></ul></li><li>Flight Controller: Custom controller.<ul><li>Based on Raspberry PI Pico.</li><li>3 axis Gyro, and 3 axis acclereometer.</li></ul></li><li>Camera: Intel RealSense D453<ul><li>Stereoscopic Depth Sensing</li><li>&lt; 2% Error Within 2m</li></ul></li><li>Companion Computer: Jetson Nano<ul><li>Quad-core AMD Cortex</li><li>4GB Onboard Memory</li><li>128 Cuda Cores</li></ul></li><li>Infrared sensor: TBD<ul><li>Used to find precise distance from ground to see if landing area is safe.</li></ul></li><li>Camera rotator/gimbal<ul><li>Will rotate camera from forward-facing to downwards to ensure safe landing area.</li><li>Stabilization of camera during flight to minimize noise in optical data</li></ul></li><li>Parcel container<ul><li>Structure<ul><li>Minimize impact on aerodynamic performance</li><li>Safe to access for users</li></ul></li><li>Food Preservation<ul><li>Keep food hot, or cold, to ensure minimal loss in quality during delivery</li></ul></li></ul></li></ul><h3 id=drone-software>Drone software</h3><ul><li><a href=https://px4.io/software/software-overview/>PX4</a>: PX4 is the firmware that runs on the Pixhawk 6c. It controls and recieves data all of the motors and sensors attached.</li><li><a href=http://qgroundcontrol.com/>QGroundControl</a>: QGroundControl is the application used to connect to, configure, and program the drone to fly autonomously.</li><li>Robot Operating System (ROS):</li><li>MAVSDK:</li><li>MAVLink:</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-8aac8c831dd69d6d6b31226a44e9fd85>3.1 - Fall 2022 Progress Updates</h1><div class=lead>A general summary of all the progress that was made in the Fall of 2022.</div><img src=/wiki/active-projects/drone-delivery/images/nav_idea.png alt=idea width=400><h3 id=pre-flight-planning>Pre-flight Planning</h3><p>The idea was to generate a occupancy grid of campus that can be used as an occupancy grid.</p><ul><li>Used OpenStreetMap to generate a isolated 3d map of campus.<ul><li><a href="https://www.openstreetmap.org/#map=5/38.007/-95.844">https://www.openstreetmap.org/#map=5/38.007/-95.844</a></li></ul></li><li>Converted that into a 2D map using Rasterization techniques.</li><li>Initially planned on using Octrees, but later decided not to due to the vast amount of memory that would consume.</li><li>Has a separate page, which describes the stack in much greater detail.
/wiki/active-projects/drone-delivery/images/
The following is the point cloud generated from OpenStreetMap.
<img src=/wiki/active-projects/drone-delivery/images/point_cloud.png alt=cloud width=400></li></ul><p>The Octree that was generated.
<img src=/wiki/active-projects/drone-delivery/images/octree.png alt=tree width=400></p><p>The occupancy gird that was generated.
<img src=/wiki/active-projects/drone-delivery/images/occupancy.png alt=occupancy width=400></p><h3 id=obstacle-avoidance>Obstacle Avoidance:</h3><ul><li>Migrate the code for Realsense camera from Python → C++<ul><li>Able to get depth matrix</li><li>Can convert to occupancy matrix</li></ul></li><li>Implimenting a the D* and A* algorithms.<ul><li>There are issues in how the paths are generated, in that sometimes diagonal paths are taken over straight ones, even though the latter is possible to produce.</li><li>To be improved in Spring 2023.</li></ul></li><li>OpenCV integration for detecting obstacles at a greater depth.</li></ul><p>The Output from the A* algorithm we develped:
<img src=/wiki/active-projects/drone-delivery/images/astar_path.png alt=astar width=400></p><p>An example of an unideal path:
<img src=/wiki/active-projects/drone-delivery/images/sub_optimal.png alt=erros width=400></p><h3 id=hardware>Hardware:</h3><ul><li>Established mission objectives, such as flight time and pay load weight.</li><li>Picked a drone kit, selected baterries, motor controllers, motors and propellers.<ul><li>A detailed document describing all the design choices that were made:
<a href="https://docs.google.com/document/d/1YcgpvD2AsxBpSHcqvRHN5PRzlyk0kKi2nl-Srrc2DXE/edit?usp=sharing">https://docs.google.com/document/d/1YcgpvD2AsxBpSHcqvRHN5PRzlyk0kKi2nl-Srrc2DXE/edit?usp=sharing</a></li></ul></li><li>Check the main page for details on what hardware components were selected.</li></ul><h3 id=interfacing>Interfacing:</h3><ul><li>Repaired the old drone, and interfaced a computer with it.</li><li>Simulated pre-programmed fight paths using Gazebo and QGroundControl.</li><li>Worked on importing OpenStreetMap data into Gazebo.</li></ul><img src=/wiki/active-projects/drone-delivery/images/sim.png alt=sim width=400><h3 id=research>Research:</h3><ul><li>Over the course of the semester, the team reseached several important topics and developed whitepapers, and plan documents.<ul><li>Deegan Osmundson: A* based navigation algorithms.
<a href=https://drive.google.com/file/d/1XcB0w0IvobgjAYehDYUqe3qoPYX0miRA/view>https://drive.google.com/file/d/1XcB0w0IvobgjAYehDYUqe3qoPYX0miRA/view</a></li><li>Seth Deegan: Drone Delivery Tech Stack
<a href="https://docs.google.com/document/d/1ekadDu0ogtgF6m-fK_AaudN6HzlPpPMSJ_rsvxW74-M/edit#heading=h.fliy5digh3xk">https://docs.google.com/document/d/1ekadDu0ogtgF6m-fK_AaudN6HzlPpPMSJ_rsvxW74-M/edit#heading=h.fliy5digh3xk</a></li><li>Sooraj Chetput: Steps in implimenting the software stack for DD.
<a href=https://drive.google.com/file/d/1RyhzLklxlVaUF0ReHZfJ17z1Qm90ic8P/view>https://drive.google.com/file/d/1RyhzLklxlVaUF0ReHZfJ17z1Qm90ic8P/view</a></li><li>Jake Harrelson and several Authors: Design and Implementation of Unmanned Aerial Vehicle for Local Food Delivery
<a href=https://docs.google.com/document/d/1YcgpvD2AsxBpSHcqvRHN5PRzlyk0kKi2nl-Srrc2DXE/view>https://docs.google.com/document/d/1YcgpvD2AsxBpSHcqvRHN5PRzlyk0kKi2nl-Srrc2DXE/view</a></li></ul></li></ul><h3 id=acknowledgements>Acknowledgements:</h3><ul><li>Project Managers: Sooraj Chetput</li><li>Obstacle Avoidance Team Members: Guna Avula (Lead), Deegan Osmundson, Chris Qiu, Ethan Baird, Mouli Sangita</li><li>Pre-flight planning: Seth Deegan (Lead), Vincent Wang</li><li>Research: Sreevickrant Sreekanth (Lead), Vignesh Charapalli</li><li>Hardware: Jacob Harrelson (Lead), Evan Zher</li><li>Interfacing: Sooraj Chetput (Lead), Atharva Bhide</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-81f9cbdbe61c3f4f5fcc1f73b61ccff9>3.2 - Pre-flight path planning</h1><div class=lead>How we plan the path the drone will take before it takes flight</div><h2 id=why>Why</h2><p>The drone should fly a path known not to have obstacles so that we can minimize the amount of time avoiding obstacles during its flight.</p><h2 id=getting-the-obstacle-data>Getting the obstacle data</h2><p>We can get data of obstacles we want to avoid like buildings, trees, and lightposts from a geospatial data source.</p><p>The geospatial data source we decided to use for our initial implementation was <a href=https://www.openstreetmap.org>OpenStreetMap</a>. OpenStreetMap is a free-to-use map of the world that can be contributed to by anyone. A large variety of features can be mapped in OpenStreetMap including all of the obstacles we need to avoid: buildings, trees, and lightposts. Because OpenStreetMap data is free, can be updated by ourselves if data is missing, and has all the obstacles we need to avoid, it is a clear choice over Google Maps which is costly, cannot be updated, and does not have data on eveything we need to avoid.</p><p>OpenStreetMap data is is made up of features. There are 3 types of features: points, lines, and areas. Lines and areas are ordered lists of points. Each point has a laditude and longitude and thus determines the position of a feature or the shape of a line or area in the world. To describe the type of feature such as &ldquo;building&rdquo; and attributes that feature has such as &ldquo;levels&rdquo; or &ldquo;color&rdquo;, every feature has a list of key-value pairs (a map/dictionary).</p><p>We only want to get all of the buildings, trees, and streetlight features and not any of the other features in an area in OpenStreetMap, so we need to filter the features in a selected area by their tags.</p><p>We can use the Overpass API querying service to get only these features. The Overpass API can be easily interfaced through by using <a href=https://overpass-turbo.eu/>Overpass Turbo</a>. We can easily create a query clicking the <code>Wizard</code> button at the top and inputting the tags we want.</p><p>Every building in OSM has a tag with the key <code>building</code>. The value of the <code>building</code> tag determines the type of the building. For example, <code>building=school</code>. We don&rsquo;t care about the type of the building so we can just add <code>building=*</code> to get all of the buildings that have a tag with the key <code>buildling</code> regardless of its value.</p><p>OSM allows users to document map individual parts of buildings to distinguish which parts have different attributes. For example, one part of a building might have more levels than another. The key for getting these &ldquo;building part&rdquo; features is <code>building:part</code>. So, we can add <code>or building:part=*</code> to also select all of these building parts.</p><p>Finally, we want to get all of the trees. The tag for a tree is <code>natural=tree</code> so we can add <code>or natural=tree</code> to also select all of the trees.</p><p>So, our final input inside of the query wizard should be <code>building=* or building:part=* or natural=tree</code>. You can then click <code>Build query</code> and the actual Overpass Query Language query text will appear on the left.</p><p>To select the area of data you want to get, click the picture icon in the upper left of the map and click adjust the box on the map to &ldquo;manually select the bbox&rdquo;.</p><p>Now, we click <code>Run</code> in the upper right to run the query and return the data we need.</p><blockquote><p>Note, you might have selected a large area and it will give you a warning you are returning a lot of data. Click <code>continue anyway</code>. You can minimize the impact this will have on your computer by disabling the results from showing on the map. This is done by removing the <code>>; out skel qt;</code> at the end of the query and re-running it.</p></blockquote><p>You can browse the data returned visually on the map or by clicking the <code>Data</code> button in the upper right.</p><p>The next step is to export the data. This can be done by clicking the <code>Export</code> button at the top and selecting which file format to export to. What file format should be exported is dependent on the type of occupancy matrix needed. Read the following section to understand what an occupancy matrix is and how it will help us find a path.</p><h2 id=creating-an-occupancy-matrix>Creating an occupancy matrix</h2><p>The data outputted from OSM is a list of shapes and points and their locations. We want to run a path planning algorithm on this data. We cannot do this though with the data in this format. Path planning algorithms require a graph to traverse (see how this works <a href=https://www.redblobgames.com/pathfinding/a-star/introduction.html>here</a>). The best way to convert multi-dimensional space into a graph is by breaking it up into square chunks where each chunk is a node in the graph. One structure these chunks can be broken into is a 2D or 3D matrix. Another structure is a <a href=https://en.wikipedia.org/wiki/Quadtree>quadtree</a> (2D) or <a href=http://www.open3d.org/docs/latest/tutorial/geometry/octree.html>octree</a> (3D). If a node in each type of matrix intersects an obstacle, we give it a value of 1 (occupied). Otherwise it has a value of 0. This is where the name &ldquo;occupancy matrix&rdquo; comes from. We can then take all of the nodes that are not occupied and link them together to create a graph that the path finding algorithm can traverse.</p><p>So, we need a solution that will convert our OSM data to an occupancy matrix.</p><h2 id=creating-a-3d-octree>Creating a 3D octree</h2><p>The first occupancy matrix we pursued creating was a 3D one.</p><p>We first researched if there were any libraries that could convert a 3D scene file like a .gltf to an occupancy matrix. We found a MATLAB function that could do this for us however we decided not to use this as it would lock us into using MATLAB&rsquo;s technology and if in the future we wanted to deploy this, we would have to pay MATLAB for computation. Therefore, we pushed on to try to find an open-source library that could do this. We ended up finding Open3D, a library that can convert a mesh like .gltf to a point cloud and then convert the point cloud to an octree. At the same time, we also found that the library OSM2World included a Command Line Interface that could convert a .osm file to a .gltf file. So, we used both tools together and ended up with a beautiful octree of campus!</p><h2 id=creating-a-2d-matrix>Creating a 2D matrix</h2><p>We were anticipating that the 3D occupancy matrix would be used in the initial flight planning and testing of the drone, however this was determined to be unecessary after we learned that we would would not be flying over buildings or trees for the first tests of the drone. Because we will not be flying over them, we could just use a 2D matrix that would document where every obstacle is located regardless of their height and thus the path planned would avoid flying over any of those obstacles.</p><p>Researching libraries that could do this for us, we found an R library that could take GeoJSON and output a matrix. However, we soon realized that the process of taking 2D shapes and converting them to a matrix is identical to the rasterization process your computer does when it is given the points of a shape like text and needs to calculate which pixels should be colored to show that text. So, we refined our search to look for libraries that could rasterize geospatail files. We finally found the Python library <a href=https://rasterio.readthedocs.io/en/latest/>Rasterio</a> that could rasterize GeoJSON into a TIFF file (an image file). All we needed to do is change the resolution of the image to fit our expected node dimensions.</p><p>The resulting converter created is located at this repository.</p><h2 id=future-plans>Future plans</h2><p>In the future we are going to use <a href=https://en.wikipedia.org/wiki/Photogrammetry>photogrammetry</a> for our obstacle data source. Photogrammetry is the technology used to generate 3D buildings in Google Maps. It is generated by taking images of features from multiple angles and the location and direction they were taken to create a 3D mesh.</p><p>Photogrammetry can be collected by flying our drone around its operating area and using images captured to generate photogrammetry.</p><p>The advantage of photogrametry over using OpenStreetMap data is it much more precise and easier to keep up-to-date than OpenStreetMap data. Photogrammetery can outline the precise geometries and locations of all obstacles in an area whereas OpenStreetMap can only document features to such detail and requires manually editing the map to document it. Photogrametry can also be much easier to keep up-to-date than OpenStreetMap. If a temporary construction area is setup where the drone typically flies, the drone can forward the imagery collected while it avoided the newly-found obstacle to the photogrametry world server to create fresh set of obstacle meshes that can will be accounted in the calculation of the flight path the next time a flight is planned through that area.</p><p>The reason photogrammetry was not chosen initially as the obstacle dataset was that we did not have a functional drone at the time of developing it to take the images used for photogrammetry.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2eccd07d908e3c28fb3ef2ed368c6290>3.3 -</h1></div><div class=td-content style=page-break-before:always><h1 id=pg-979cc4dd70e1b73a24f4283657830a48>4 - Robot Arm</h1><div class=lead>Building a open-source, 3d-printed manipulator</div><h2 id=goal>Goal</h2><h3 id=mission>Mission</h3><p>Our overarching goal is to explore why robots are limited to factory environments and why we don&rsquo;t have them cooking for us and folding our clothes yet.</p><p>In our journey, we plan to publish our progress, tutorials, code, and workshops.</p><h3 id=right-now>Right now</h3><p>Right now, we&rsquo;re building software/hardware for a robot arm to play chess.</p><p>From this, we hope to validate fundamental understanding of robot arm design, control (using control systems and/or reinforcement learning), sensor systems (vision, encoders, tactile), and mechanical actuator systems (servos, steppers, gearboxes).</p><h4 id=hardware>Hardware</h4><p>We currently have two hardware efforts:</p><ol><li>Designing a <a href=https://wiki.purduearc.com/wiki/robot-arm/hardware#mr-janktastic>Mr. Janktastic</a> from scratch</li><li>Building the <a href=https://wiki.purduearc.com/wiki/robot-arm/hardware#bcn3d-moveo-arm>open-source BCN3D Moveo Arm</a> for a better hardware system for software to experiment and test vision, RL systems with.</li></ol><h4 id=software>Software</h4><p>Working on a variety of problems in vision, control, and high level planning (RL soon!)). See the <a href=https://wiki.purduearc.com/wiki/robot-arm/software>Software Docs</a> for a deeper dive.</p><h2 id=what-have-we-done>What have we done?</h2><h3 id=may-2021>May 2021</h3><ul><li>Object detection working on chess pieces with 90%+ accuracy using YOLOv5 and usable in ROS<ul><li>Put together a 500+ chess piece dataset for detection</li></ul></li></ul><img src=images/obj_det_may_21.png alt="Object detection demo" width=400><ul><li>Prototype gripper fingers that can pick up chess pieces decently well</li></ul><p>{% include googleDrivePlayer.html id=&ldquo;1P8rwWDJa1Yuv88X697RMvEq04j1IgpqW/preview&rdquo; %}</p><ul><li>Created Gazebo simulation that is controlled by MoveIt pipeline, including a simulated camera, chessboard, and chess pieces</li></ul><p>{% include googleDrivePlayer.html id=&ldquo;19FZ7lsqCn6DEChjjdBgsTy0feUANYaVO/preview&rdquo; %}</p><h3 id=december-2020>December 2020</h3><ul><li>Got an early version of protoarm to stack some boxes using IK and trajectory planning from MoveIt and ROS</li></ul><p>{% include googleDrivePlayer.html id=&ldquo;1yms3OuqYp-n4JCt-yBGZg8yEyajXOj_M/preview&rdquo; %}</p><ul><li>Designed a prototype 6dof arm</li></ul><img src=images/6dof_dec_20.png alt="6DOF arm CAD" width=200><h2 id=quick-links>Quick links</h2><ul><li><a href=https://github.com/purdue-arc/arc_robot_arm>GitHub</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3cf4f3bfb079be3d11ea57b6fedc725b>4.1 - Software</h1><div class=lead>Learn about the robot arm software stack</div><blockquote><p>If you want to look at code and run some of it, check out <a href=https://github.com/purdue-arc/arc_robot_arm>arc_robot_arm</a> README for quick start details and usage instructions.</p></blockquote><h2 id=how-to-use-this-if-youre-new>How to use this if you&rsquo;re new</h2><p>From the diagram and subsection descriptions, you can hopefully get a preliminary high-level understanding of each of the parts and how they fit together.</p><p>From this, you can then:</p><ol><li>Choose which subsection interests you most</li><li>Follow links in the specified subsection to get more contextual understanding (Optionally ask questions in the Discord, hop in a voice chat, etc for more fun convo about the area)</li><li>Ensure you understand the related topics for each subsections with practical understanding/experience by completing a step-by-step tutorial (either use google to find one or ask for suggestions in Discord), ensuring that it uses the related topics (<strong>Can be done concurrently with #2</strong>)</li><li>At this point, you should have enough knowledge/experience in your subsection to contribute to the current approach and possibly even change what the current approach is if you find better ways to do things (if so, update these docs!)</li></ol><h2 id=system-overview>System Overview</h2><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph
  subgraph Behavior Planning 
    BP1[Behavior Planner]
    BP2[Chess Engine or Human]
    %% BP3[RL Policy]
  end
  subgraph Low Level
    direction LR
    LL1[Protoarm ROS Controller]
    LL2[driver]
    LL3[servos] 
  end
  subgraph Simulation
    direction LR
    S1[Gazebo ROS Controller]
    S2[Simulated robot joints from URDF] 
  end
  subgraph Vision 
    direction LR
    V1[YOLOv5 Object Detector ROS node]
    V2[Chessboard Detector] 
  end
  subgraph Planning/Control
    PC1[Visual Servoing ROS node] 
    PC2[MoveTo C++ ROS node]
    PC3[MoveIt] 
  end

  %% Low level
  LL2 -- servo position PWM --&gt; LL3
  LL3 -- potentiometer feedback \- not done --&gt; LL2
  LL1 --&gt; LL2

  %% Behavior Planning
  BP2 --&gt; BP1 
  BP1 --&gt; PC1 

  %% Vision 
  V2 --&gt; BP1 
  V1 --&gt; PC1

  %% Simulation 
  S1 --&gt; S2
  
  %% Planning/Control 
  PC1 --&gt; PC2
  PC2 --&gt; PC3
  PC3 --&gt; S1
  PC3 --&gt; LL1

  %% Links
  click LL1 href &#34;http://wiki.purduearc.com/wiki/robot-arm/software#controller&#34; &#34;Sensors&#34;
  click LL2 href &#34;http://wiki.purduearc.com/wiki/robot-arm/software#driver&#34; &#34;Sensors&#34;

  click PC1 href &#34;http://wiki.purduearc.com/wiki/robot-arm/software#visual-servoing&#34; &#34;Sensors&#34;
  click PC2 href &#34;http://wiki.purduearc.com/wiki/robot-arm/software#kinematics-and-planning&#34; &#34;Sensors&#34;
  click PC3 &#34;https://moveit.ros.org/assets/images/diagrams/moveit_pipeline.png&#34; &#34;Open this in a new tab&#34; _blank

  click BP1 href &#34;http://wiki.purduearc.com/wiki/robot-arm/software#behavior-planner&#34; &#34;Sensors&#34;

  click S1 href &#34;http://wiki.purduearc.com/wiki/robot-arm/software#simulation&#34; &#34;Sensors&#34;
  click S2 href &#34;http://wiki.purduearc.com/wiki/robot-arm/software#simulation&#34; &#34;Sensors&#34;

  click V1 href &#34;http://wiki.purduearc.com/wiki/robot-arm/software#chess-piece-detection&#34; &#34;Sensors&#34;
  click V2 href &#34;http://wiki.purduearc.com/wiki/robot-arm/software#chessboard-detection&#34; &#34;Sensors&#34;

  classDef not_started fill:#ff8181
  classDef in_progress fill:#ffba82
  classDef done fill:#81ff9b
  class BP1,LL1,BP2,BP3,V2 not_started
  class PC1 in_progress
  class S1,S2,LL2,LL3,V1,PC2,PC3 done
</code></pre><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph
  l1[Not Started]
  l2[In Progress]
  l3[Done]
classDef not_started fill:#ff8181
classDef in_progress fill:#ffba82
classDef done fill:#81ff9b
class l1 not_started
class l2 in_progress
class l3 done
</code></pre><h2 id=high-level>High level</h2><h3 id=behavior-planner>Behavior Planner</h3><p>After the robot turns on or at any given point of time, what should the robot do? This is the job of the behavior planner.</p><p>Overall, it should output executable commands that return true or false if they are completed successfully and then output the next command. A preliminary decision flowchart that a robot can make is modeled here:</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph
  a1[Scanning for change in board state]
  p1[Virtual Human]
  p2[Engine]
  a3[Identify and pick up piece]
  a4[Identify destination and place piece]
  a1 -- 2d picture of board --&gt; p1
  a1 -- FEN Notation of board--&gt; p2
  p1 -- Next move --&gt; a3
  p2 -- Next move --&gt; a3
  a3 --&gt; a4 --&gt; a1
</code></pre><p>To implement the behavior planner, a Finite State Machine (FSM) and/or a Behavior Tree (BT) can be used, which both have tradeoffs in <strong>modularity</strong> and <strong>reactivity</strong>, (<a href=https://roboticseabass.com/2021/05/08/introduction-to-behavior-trees/>read more</a>, scroll to the last section for ).</p><p><strong>Related Topics:</strong></p><ul><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#ros>ROS</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#pythonnumpy>Python</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#fsm>FSM</a> and/or <a href=https://wiki.purduearc.com/wiki/robot-arm/software#behavior-trees>BT</a></li></ul><h2 id=vision>Vision</h2><h3 id=chess-piece-detection>Chess Piece Detection</h3><p>How does our robot determine which piece is which and where is that piece relative to the arm? This is a common scenario in many real-world use cases and object detection, as the name suggests, is used to detect the chess pieces.</p><p>The object detection stack consists of the <a href=https://github.com/ultralytics/yolov5>YOLOv5</a> object detection model trained on a custom dataset and outputs 2d bounding boxes. This can then be extrapolated to 3D coordinates from the camera intrinsics relative to the arm with some coordinate transformations from camera -> robot arm base.</p><p>For chess, the model is trained with a custom large chess piece dataset of 500+ images stored on <a href=https://app.roboflow.com/>roboflow</a>. <a href="https://colab.research.google.com/drive/1XJ82eVA0cEfTczXFMtV1sunqvsfiJWQ0?usp=sharing">Here</a> is a simple colab that walks through the training process using a Roboflow dataset.</p><p>Inference is using the <code>yolov5_pytorch_ros</code> ROS package using the <code>detector</code> ROS node using Python/PyTorch, allowing it to communicate detections and classes to other nodes such as for visual servoing. It reaches around 15-20 FPS without a GPU on a Mac.</p><blockquote><p>See <a href=https://github.com/purdue-arc/arc_robot_arm/tree/main/chess_piece_detector>chess_piece_detector</a> for quick start details and usage instructions.</p></blockquote><p><strong>Related Topics:</strong></p><ul><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#ros>ROS</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#pythonnumpy>Python</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#object-detection>Object Detection using YOLOv5</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#coordinate-transforms>Coordinate Transformations</a></li></ul><h3 id=chessboard-detection>Chessboard Detection</h3><p>Playing chess is more than just picking and placing pieces. The robot needs to actually beat the human. We need to know the exact state of the chessboard, so another person or an overpowered engine can say what move to play next.</p><p>We can do this using a computer vision techniques with an image of the current chessboard as an input and the FEN notation of to board as an output. The following diagram shows this visually:</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph LR
  a1[Real chessboard] --&gt; a2[2D chessboard] 
  a2 --&gt; a3[FEN Notation] 
  a3 --&gt; a4[Chess Engine]
  a2 --&gt; a5[Virtual human player]
</code></pre><p>Some external projects we plan to use to complete the above:</p><ul><li><a href=https://github.com/maciejczyzewski/neural-chessboard>Real chessboard -> 2D chessboard</a></li><li><a href=https://github.com/Elucidation/tensorflow_chessbot>2D chessboard -> FEN Notation</a></li></ul><p><strong>Related Topics:</strong></p><ul><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#ros>ROS</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#pythonnumpy>Python</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#machine-learning>Machine Learning</a></li></ul><h2 id=control>Control</h2><h3 id=visual-servoing>Visual Servoing</h3><p>Even if we can see where the pieces are with the camera, how does the robot arm move closer to the chess piece it wants to pick up? Without visual servoing, any error that the robot makes cannot be accounted for and adjusted for accordingly, resulting in lots of fails. Visual servoing is a control algorithm using images as input to control for any errors that the robot makes.</p><p>We are using image-based visual servoing to localize the robot arm hand over an object in a graspable configuration, using images/2d bounding boxes as inputs to the system and servo commands as outputs.</p><img src=/wiki/active-projects/robot-arm/images/vs-diagram.png alt="Diagram of controller" width=400>
> Visual servoing system diagram ([source](https://www.researchgate.net/figure/Visual-servoing-closed-loop_fig1_265436696))<p>See the <code>protoarm_visual_servoing</code> package for more details.</p><p><strong>Related Topics:</strong></p><ul><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#ros>ROS</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#pythonnumpy>Python</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#visual-servoing-1>Visual Servoing</a></li><li>Usage of the MoveTo node</li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#coordinate-transforms>Coordinate Transforms</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#robot-arm-kinematics>Robot Arm Kinematics</a></li></ul><h3 id=kinematics-and-planning>Kinematics and Planning</h3><p>As a human, it is simple to move our joints and pick something up in 3D space. For a robot arm, it knows nothing of 3D space, only numerical angles for each of its joints. So, we use <a href=https://en.wikipedia.org/wiki/Inverse_kinematics>inverse kinematics</a> (IK), the mathematical process of converting 3D space coordinates to joint angles, to determine the final position for the robot arm.</p><p>Now what happens between the start and final position? That&rsquo;s the job of the motion planner. It determines a safe collision-free trajectory for each joint and creates the plan. Then, the plan is executed, either in real life or in simulation.</p><p>As of now, all the kinematics and planning heavy lifting is done by <a href=http://moveit.ros.org/>MoveIt</a>. The <code>protoarm_kinematics</code> package houses a wrapper written in C++ that abstracts the process of sending <a href=http://docs.ros.org/en/melodic/api/sensor_msgs/html/msg/JointState.html>JointState</a> or <a href=http://docs.ros.org/en/noetic/api/geometry_msgs/html/msg/Pose.html>Pose</a> goals.</p><p>The wrapper is interfaced externally using the <code>move_to</code> node. Refer to the <a href=https://github.com/purdue-arc/arc_robot_arm/blob/main/protoarm_kinematics/src/test_kinematics>test_kinematics</a> rospy file in <code>protoarm_kinematics/src</code> for usage of the <code>move_to</code> node.</p><p>This package is also where we would keep our custom kinematics plugin and planning library if we choose to make it from scratch, instead of the default <a href=https://ros-planning.github.io/moveit_tutorials/doc/kinematics_configuration/kinematics_configuration_tutorial.html#the-kdl-kinematics-plugin>KDL</a> kinematics plugin and <a href=https://ros-planning.github.io/moveit_tutorials/doc/ompl_interface/ompl_interface_tutorial.html>OMPL</a> motion planning library.</p><blockquote><p>See <a href=https://github.com/purdue-arc/arc_robot_arm/tree/main/protoarm_kinematics>protoarm_kinematics</a> for quick start details and usage instructions.</p></blockquote><p><strong>Related Topics:</strong></p><ul><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#ros>ROS</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#pythonnumpy>Python</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#moveit>MoveIt</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#coordinate-transforms>Coordinate Transforms</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#robot-arm-kinematics>Robot Arm Kinematics</a></li></ul><h2 id=low-level>Low level</h2><h3 id=controller>Controller</h3><p><code>protoarm_control</code> is the ROS package that ensures that MoveIt execution commands are executed exactly as expected and to a degree of certainty in the real world.</p><p>Right now, the driver communicates directly with MoveIt as the <code>protoarm_control</code> package doesn&rsquo;t exist yet and because we do not have servo feedback.</p><p>Given that the protoarm uses some of the cheapest servos on the market, how can we still get dependable sub-millimeter precision to do tasks reliably? One way to do so is to hack our servos to add encoders, and use a control scheme that can take velocity and torque into account.</p><p>Inspired by Adam&rsquo;s <a href=https://github.com/adamb314/ServoProject>Servo Project</a>.</p><p><strong>Related Topics:</strong></p><ul><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#ros>ROS</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#pythonnumpy>Python</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#cascade-style-control>Cascade-style Control</a></li><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#encoders>Encoders</a></li></ul><h3 id=driver>Driver</h3><p>The <code>protoarm_driver</code> is written in Arduino that actually interfaces with the servos and encoders. It sets joint limits, does coordinate frame conversions from the URDF to the actual robot, converts MoveIt angles to servo joint angles $$(-\pi,\pi)$$ to $$(0^\circ,180^\circ)$$, and executes servo commands using PWM to the servos.</p><p><strong>Related Topics:</strong></p><ul><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#arduino>Arduino</a></li></ul><h2 id=simulation>Simulation</h2><h3 id=gazebo>Gazebo</h3><p>There are lots of benefits from simulation spanning from speeding up development and testing of software, realistic environments for reinforcement learning, and testing proof of concepts.</p><p>Our arm is simulated in Gazebo with a Realsense D435 camera (<code>realsense_ros_gazebo</code>), chessboard, and chess pieces (<code>chessboard_gazebo</code>). The robot arm and the camera are represented in <a href=http://wiki.ros.org/urdf/XML/model>URDF</a>.</p><p>Sensors, actuation, gazebo plugins, and more are <a href=http://wiki.ros.org/urdf/XML>specifications</a> that can be added. For the arm, these specifications exist in the <code>.xacro</code> files in the <code>urdf</code> folder of the <code>protoarm_description</code> ROS package.</p><p>Gazebo uses SDF models (in <code>models</code> folder of <code>chessboard_gazebo</code>) to represent static assets in the simulation like the chess pieces and chessboard. These objects are then spawned into a Gazebo world (along with the robot URDF), represented with a <code>.world</code> file.</p><blockquote><p>See <a href=https://github.com/purdue-arc/arc_robot_arm/tree/main/protoarm_bringup>protoarm_bringup</a> for quick start details and usage instructions.</p></blockquote><p>{% include googleDrivePlayer.html id=&ldquo;19FZ7lsqCn6DEChjjdBgsTy0feUANYaVO/preview&rdquo; %}</p><p><strong>Related Topics:</strong></p><ul><li><a href=https://wiki.purduearc.com/wiki/robot-arm/software#gazebo-1>Gazebo</a></li><li><a href>URDF</a></li></ul><h1 id=resources>Resources</h1><h2 id=tools>Tools</h2><h3 id=ros>ROS</h3><ul><li><a href=https://wiki.purduearc.com/wiki/tutorials/snake-tutorial>ARC ROS tutorials (Recommended)</a></li><li><a href=http://wiki.ros.org/ROS/Tutorials>Official ROS tutorials</a></li></ul><h3 id=github>GitHub</h3><ul><li><a href=https://www.freecodecamp.org/news/learn-the-basics-of-git-in-under-10-minutes-da548267cc91/>Learn Git under 10 min</a></li></ul><h3 id=c>C++</h3><ul><li><a href="https://www.youtube.com/watch?v=18c3MTX0PK0&amp;list=PLlrATfBNZ98dudnM48yfGUldqGD0S4FFb">In depth tutorial playlist for C++</a><ul><li>Recommended topics:<ul><li>if/else, loops, functions, classes</li><li>Pointers/References</li><li>Smart pointers</li><li>Dynamic Arrays (std::vector)</li></ul></li></ul></li><li>Very useful numeric libraries<ul><li><a href="https://eigen.tuxfamily.org/index.php?title=Main_Page">Eigen</a>: Extremely efficient matrix math library</li></ul></li></ul><h3 id=pythonnumpy>Python/Numpy</h3><p><strong>Python</strong></p><ul><li><a href="https://www.youtube.com/watch?v=rfscVS0vtbw">Tutorial</a></li><li>Important topics to understand:<ul><li>Basics are good - variables + logic, functions, classes</li></ul></li></ul><p><strong>Numpy</strong></p><blockquote><p>Must use when working with large arrays (i.e images)</p></blockquote><ul><li><a href="https://www.youtube.com/watch?v=8Y0qQEh7dJg">Tutorial</a></li><li>Important topics to understand:<ul><li>Creating arrays</li><li>slicing + indexing</li><li>reshaping</li><li>linear algebra</li></ul></li></ul><p><strong>OpenCV</strong>:</p><blockquote><p>Use for computer vision and image transformations like color detection, tracking, etc</p></blockquote><ul><li><a href="https://www.youtube.com/watch?v=oXlwWbU8l2o">Tutorial</a></li><li>Important topics to understand:<ul><li>image transformation</li><li>thresholding</li><li>read/write images from file</li><li>resizing</li><li>tracking</li></ul></li></ul><h3 id=arduino>Arduino</h3><ul><li>Top hit for &ldquo;arduino tutorial&rdquo; on google should work</li></ul><h3 id=gazebo-1>Gazebo</h3><ul><li>Important topics (Googlable)<ul><li>Creating a world</li><li>Adding assets</li><li>ROS control in Gazebo</li><li>Cameras in Gazebo</li></ul></li></ul><h3 id=good-coding-practices>Good coding practices</h3><ul><li><a href=https://cpp-optimizations.netlify.app/>Optimized C++ code</a></li></ul><p>TODO</p><h2 id=algorithms-theory-math>Algorithms, Theory, Math</h2><h3 id=coordinate-transforms>Coordinate Transforms</h3><ul><li><a href=http://faculty.salina.k-state.edu/tim/robotics_sg/Pose/Pose.html>Understanding the pose of a robot</a></li></ul><h3 id=fsms>FSMs</h3><ul><li><a href=https://www.clear.rice.edu/engi128/Handouts/Lec17-Robotics.pdf>tutorial slideshow</a></li></ul><h3 id=behavior-trees>Behavior Trees</h3><ul><li><a href=https://roboticseabass.com/2021/05/08/introduction-to-behavior-trees/>intro to behavior trees</a></li></ul><h3 id=control-algorithms>Control Algorithms</h3><h4 id=pid-controller>PID Controller</h4><ul><li><a href="https://ctms.engin.umich.edu/CTMS/index.php?example=Introduction&amp;section=ControlPID">tutorial</a></li></ul><h4 id=cascade-controller>Cascade Controller</h4><ul><li><a href=https://github.com/adamb314/ServoProject>Implememtation by Adam for Servo Project</a></li></ul><h3 id=ai>AI</h3><h4 id=object-detection>Object Detection</h4><ul><li><a href=https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/>train yolov5 model on custom dataset</a></li></ul><h4 id=machine-learning>Machine Learning</h4><h2 id=sensor-related>Sensor related</h2><h3 id=camera>Camera</h3><ul><li><a href>Camera driver ROS package</a></li><li><a href>Camera calibration ROS package</a></li></ul><h3 id=encoders>Encoders</h3><ul><li><a href=https://www.arduino.cc/reference/en/libraries/encoder/>Arduino Encoder Package</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-bbd7a74090357072550611c9bd860e2e>4.2 - Hardware</h1><div class=lead>Learn about the robot arm hardware</div><h2 id=meet-the-robots>Meet the robots</h2><h3 id=protoarm>Protoarm</h3><p>This is protoarm (short for prototype arm), a 5-DOF robot arm adapted slightly from <a href="https://www.youtube.com/watch?v=_B3gWd3A_SI">HowToMechatronics&rsquo; model</a> that we built first to understand ROS, MoveIt, and the software stack for robot arms.</p><img src=/wiki/active-projects/robot-arm/images/protoarm.png alt="Protoarm CAD" width=400><h3 id=bcn3d-moveo-arm>BCN3D Moveo Arm</h3><p>This is the <a href=https://www.bcn3d.com/bcn3d-moveo-the-future-of-learning-robotic-arm/>BCN3D Moveo</a> arm that is fully open-source, built with steppers rather than cheap servos which have feedback control (a step up from the Protoarm), and is actually big enough to reach a full size chessboard. It also is much better designed and assembled than the Protoarm.</p><p>Quick links:</p><ul><li><a href=https://github.com/BradyHooverDesigns/BCN3D-MOVEO-BOM>BOM</a></li><li><a href=https://github.com/BCN3D/BCN3D-Moveo>CAD/Instructions</a></li></ul><img src=/wiki/active-projects/robot-arm/images/moveo.jpg alt=Moveo width=400><h3 id=mr-janktastic>Mr. Janktastic</h3><p>Building a 6-DOF robot arm fully from scratch with Dynamixel Servos.</p><img src=/wiki/active-projects/robot-arm/images/janktastic.png alt=Moveo width=400><h2 id=resources>Resources</h2><h3 id=cad>CAD</h3><p>TODO</p><h3 id=manufacturing>Manufacturing</h3><h4 id=3d-printing>3D Printing</h4><p>TODO</p><h4 id=metal-manufacturing-tools>Metal Manufacturing tools</h4><p>TODO</p><h4 id=laser-cutting>Laser Cutting</h4><p>TODO</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5a81c8e00f9a700ac0041333f54dc646>4.3 -</h1></div><div class=td-content style=page-break-before:always><h1 id=pg-a6210bbefa7cfc9afc14edbf00e77cf4>5 - Wizard's Chess</h1><div class=lead>Building a giant, automated chess board</div><h2 id=goal>Goal</h2><p>Our goal for this project is to be able to create a fully autonomous, life sized chess game that people will be able to play using only voice commands. This project was inspired by the Wizard&rsquo;s Chess game in Harry Potter and the Philosopher&rsquo;s Stone. We will be exploring different hardware and software concepts, including engineering design, CAD, manufacturing, computer vision, and trajectory planning.</p><h2 id=subsystems>Subsystems</h2><h3 id=hardware>Hardware</h3><p>We are currently working on developing all 32 pieces for the game, as well as manufacturing the chess board ourselves. Navigate to our [hardware docs]({% link wiki/wizards-chess/hardware.md %}) to understand where we are at in terms of our hardware progress.</p><h4 id=battery-team>Battery Team</h4><p>The batteries team has been researching what types of batteries we need to power the robot as well as how to do it. Their mission is to successfully provide the amount of power to allow a full game of chess to run without needing any recharges. Currently, the upper bound that they have set for one full game is 2 hours.</p><h4 id=cad>CAD</h4><p>The CAD team is utilizing Fusion 360 to create a model of the basic chess piece. They are also our go-to team for laser cutting, 3D printing, CNCing, and general manufacturing, especially when it comes to needing certain files.</p><h3 id=software>Software</h3><p>We will be working on vision using apriltags to determine the positioning of each of the pieces on the board on an x,y coordinate plane and voice inputs to direct the pieces where to go. This will all be controlled by the overarching Raspberry Pi 4 controller, intakes voice commands, determines valid moves, and calculates the correct trajectory for each of the pieces. Navigate to our [software docs]({% link wiki/wizards-chess/software.md %}) to get a deeper dive into these concepts.</p><h4 id=computer-vision>Computer Vision</h4><p>This team is tasked with figuring out how to capture the position of all pieces on the board at all times. The current design is to use an overhead camera that will encompass the whole board, and then use Apriltags on each robot and 4 corners. With this information, we will be able to digitally map them on an xy-coordinate plane and perform the trajectory and pathing calculations correctly in order to move the correct chess pieces to the correct spots.</p><h4 id=voice-recognition>Voice Recognition</h4><p>This team is tasked with implementing full voice recognition capabilities for the chess game. They are looking into different voice recognition libraries that will be the best to implement in order to assist the creation of the software needed. The voice recognition software will take different commands (ie: Knight to e4) and use the trajectory planner to convert them to a point on the plane for the piece to move to.</p><h2 id=what-have-we-been-up-to>What have we been up to?</h2><h3 id=spring-2021>Spring 2021</h3><p>This spring was our first semester on this project!</p><ul><li>Designed the triangular chess pieces<ul><li>Each robot is about 9.75" x 7.75" and laser cut from .5" plywood and each square on the field is 1.5&rsquo; by 1.5'</li></ul></li></ul><img src=images/WC-prototype1.jpg alt="Chess Piece Prototype" width=400><ul><li>Put together prototype with motor movement</li></ul><div class=embed-container><iframe width=640 height=480 src=https://drive.google.com/file/d/18uXqdxGblfsNUdoEI2qEVPu5-sYCvQ7a/preview frameborder=0 allowfullscreen></iframe></div><ul><li>Wrote chess algorithm using Python</li><li>Simulated robot movement in ROS</li></ul><h2 id=plans-for-the-future>Plans for the Future</h2><p>We are planning on creating low-poly structures for each of the chess pieces and combining them with LED indicators to distinguish between the different pieces. We will also be looking into creating custom PCBs for each of the robots.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-443684d680c960d7f4e4cc6230251124>5.1 - Software</h1><h2 id=system-overview>System Overview</h2><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph
  subgraph High Level
    HL1[Trajectory Planner]
    HL2[Voice Recognition]
  end
  subgraph Chess Piece Robot
    direction LR
    LL1[Differential Drive Controller]
    LL2[Driver]
    LL3[Motors] 
  end
  subgraph Vision 
    direction LR
    V1[Apriltag Localization]
  end
  subgraph Control
    C1[Trajectory Execution ROS nodes] 
  end
  %% Low level
  LL1 --&gt; LL2
  LL2 --&gt; LL3

  %% Behavior Planning
  HL2 --&gt; HL1
  HL1 --&gt; C1

  %% Vision 
  V1 --&gt; C1
  V1 --&gt; HL1

  %% Control 
  C1 --&gt; LL1

  %% Links
  click LL1 href &#34;http://wiki.purduearc.com/wiki/wizards-chess/software#differential-drive-controller&#34; &#34;Label&#34;
  click C1 href &#34;http://wiki.purduearc.com/wiki/wizards-chess/software#trajectory-execution-controller&#34; &#34;Label&#34;
  click HL1 href &#34;http://wiki.purduearc.com/wiki/wizards-chess/software#trajectory-planner&#34; &#34;Label&#34;
  click HL2 href &#34;http://wiki.purduearc.com/wiki/wizards-chess/software#voice-recognition&#34; &#34;Label&#34;
  click V1 href &#34;http://wiki.purduearc.com/wiki/wizards-chess/software#apriltag-localization&#34; &#34;Label&#34;

  classDef not_started fill:#ff8181
  classDef in_progress fill:#ffba82
  classDef done fill:#81ff9b
  class V1,HL1,HL2,LL1,C1 not_started
  %% class C1 in_progress
  class LL2,LL3 done
</code></pre><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph
  l1[Not Started]
  l2[In Progress]
  l3[Done]
classDef not_started fill:#ff8181
classDef in_progress fill:#ffba82
classDef done fill:#81ff9b
class l1 not_started
class l2 in_progress
class l3 done
</code></pre><h2 id=high-level>High Level</h2><h3 id=voice-recognition>Voice Recognition</h3><h4 id=problem>Problem</h4><p>Given a voice command such as &ldquo;Knight to e4&rdquo;, generate a machine readable command that can be inputted to the Trajectory Planner.</p><h4 id=potential-approach>Potential Approach</h4><p>Can use python-based speech recognition libraries to output raw text, which can easily be converted to a <a href>ROS message</a> that is consumed by the Trajectory Planner</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph LR 
  A1[Voice Input]
  A2[Speech Recognition Python Library]
  A3[ROS wrapper node]
  A4[Behavior Planner]

A1 --&gt; A2
A2 --&gt; A3
A3 --&gt; A4
</code></pre><h4 id=related>Related</h4><ul><li><a href=https://wiki.purduearc.com/wiki/wizards-chess/software#ros>ROS</a></li><li><a href=https://wiki.purduearc.com/wiki/wizards-chess/software#python>Python</a></li><li><a href=https://github.com/Uberi/speech_recognition>SpeechRecognition Python Library</a></li></ul><h3 id=trajectory-planner>Trajectory Planner</h3><h4 id=problem-1>Problem</h4><p>Given that we have just a camera which can localize (know the position of) each chess piece and find the center of any empty board square, how do we describe the movement of a chess piece from one square to another to complete a move using information we can actually execute on the robot, while not creating a collision with another piece?</p><h4 id=potential-approach-1>Potential Approach</h4><p>Since our input to the system is in pixels, we can:</p><ol><li>Query the x,y position of where the piece starts and ends</li><li>Create a time-dependent, x,y-coordinate-valued trajectory between the two x,y positions (could be as simple as a straight line)</li><li>If the above trajectory results in a collision (can be determined using the distance formula between pieces and knowing the radius of each piece), create a trajectory for the other piece to move out of the way. (Take a look at flocking algorithms, linked below)</li><li>Now that we have a set of trajectories, send them to the trajectory execution node of each respective chess piece to be executed concurrently</li></ol><h4 id=related-1>Related</h4><ul><li><a href=https://wiki.purduearc.com/wiki/wizards-chess/software#ros>ROS</a></li><li><a href=https://wiki.purduearc.com/wiki/wizards-chess/software#python>Python</a></li><li><a href=https://wiki.purduearc.com/wiki/wizards-chess/software#opencv>OpenCV</a></li><li><a href=https://codeheir.com/2021/03/27/the-flocking-algorithm/>Flocking Algorithm</a></li></ul><h2 id=vision>Vision</h2><h3 id=apriltag-localization>Apriltag Localization</h3><h4 id=problem-2>Problem</h4><p>How can we determine the position of each chess piece and every empty square on the board?</p><h4 id=potential-approach-2>Potential Approach</h4><p>Apriltags are a simple way to solve this accurately. With just an Apriltag on the four corners of the board (still need to measure square size manually) and an Apriltag on each chess piece, we can determine the xyz position of the entire system.</p><h4 id=related-2>Related</h4><ul><li><a href=https://wiki.purduearc.com/wiki/wizards-chess/software#ros>ROS</a></li><li><a href=https://github.com/AprilRobotics/apriltag_ros>Apriltag ROS library</a></li><li><a href=https://april.eecs.umich.edu/software/apriltag.html>Apriltag website</a></li></ul><h2 id=control>Control</h2><h3 id=trajectory-execution-controller-nodes>Trajectory Execution Controller Nodes</h3><h4 id=problem-3>Problem</h4><p>How do we ensure that the robot is correctly executing the trajectory given by the Trajectory Planner, and if it is inaccurate, how do we control for it?</p><h4 id=potential-approach-3>Potential Approach</h4><p>Fortunately, we can look at control theory to give us a good solution: an LQR controller. This essentially is an optimization algorithm with weighted costs that the algorithm minimizes.</p><p>Also, a Pure Pursuit algorithm may be used as well.</p><p>For any algorithm, the inputs are the trajectory itself, the current localized position of the chess piece, the target position, and the output is a <a href=http://docs.ros.org/en/api/geometry_msgs/html/msg/Twist.html>Twist</a> ROS message velocity command (the linear and angular velocity of the robot).</p><p>This would be running on the master computer that has access to camera input and the will be communicated to each robot as there will be a trajectory execution node for each robot.</p><h4 id=related-3>Related</h4><ul><li><a href=https://wiki.purduearc.com/wiki/wizards-chess/software#ros>ROS</a></li><li><a href=https://wiki.purduearc.com/wiki/wizards-chess/software#python>Python</a></li><li><a href=https://wiki.purduearc.com/wiki/wizards-chess/software#c++>C++</a></li><li><a href=https://sophistt.github.io/2020/05/02/lqr-controller-theory/>LQR Controller</a></li><li><a href=https://vinesmsuic.github.io/2020/09/29/robotics-purepersuit/#theoretical-derivation>Pure Pursuit Algorithm</a></li></ul><h2 id=robot>Robot</h2><h3 id=differential-drive-controller>Differential Drive Controller</h3><h4 id=problem-4>Problem</h4><p>The robot motors don&rsquo;t understand what Twist linear/angular velocity is, just the speed and direction at which they need to spin. So how do we convert the Twist to individual motor speeds (just a value between 0-255)?</p><h4 id=potential-approach-4>Potential Approach</h4><p>For different types of steering geometries, there are different control algorithms used to steer mobile vehicles (Skid-steer, Ackermann, differential drive, etc).</p><p>For our robot, it is a differential drive steering configuration, specific to the geometry of two wheels on either side and a caster wheel for stability.</p><p>This would be running on the robot.</p><h4 id=related-4>Related</h4><ul><li><a href=https://wiki.purduearc.com/wiki/wizards-chess/software#ros>ROS</a></li><li><a href=https://wiki.purduearc.com/wiki/wizards-chess/software#python>Python</a></li><li><a href=https://wiki.purduearc.com/wiki/wizards-chess/software#c++>Arduino/C++</a></li><li><a href=http://wiki.ros.org/diff_drive_controller>Differential Drive Controller ROS Package</a></li><li><a href="https://www.youtube.com/watch?v=aE7RQNhwnPQ&amp;list=PLp8ijpvp8iCvFDYdcXqqYU5Ibl_aOqwjr&amp;index=10">Differential Drive Robots lecture - recommended for understanding theory</a></li></ul><h2 id=problems-not-described-yet>Problems not described yet</h2><ul><li>Structuring the robot-level systems so they are modular and can easily spawned for all the chess pieces.</li><li>Ensuring that the system is controlled (i.e cameras can see all apriltags at all times, etc)</li><li>Playing against an engine</li><li>Simplifying the interface with a human (app?, website?)</li></ul><h1 id=resources>Resources</h1><h2 id=tools>Tools</h2><h3 id=ros>ROS</h3><ul><li><a href=https://wiki.purduearc.com/wiki/tutorials/snake-tutorial>ARC ROS tutorials (Recommended)</a></li><li><a href=http://wiki.ros.org/ROS/Tutorials>Official ROS tutorials</a></li></ul><h3 id=github>GitHub</h3><ul><li><a href=https://www.freecodecamp.org/news/learn-the-basics-of-git-in-under-10-minutes-da548267cc91/>Learn Git under 10 min</a></li></ul><h3 id=c>C++</h3><ul><li><a href="https://www.youtube.com/watch?v=18c3MTX0PK0&amp;list=PLlrATfBNZ98dudnM48yfGUldqGD0S4FFb">In depth tutorial playlist for C++</a><ul><li>Recommended:<ul><li>if/else, loops, functions, classes</li><li>Pointers/References</li><li>Smart pointers</li><li>Dynamic Arrays (std::vector)</li></ul></li></ul></li><li>Very useful numeric libraries<ul><li><a href="https://eigen.tuxfamily.org/index.php?title=Main_Page">Eigen</a>: Extremely efficient matrix math library</li></ul></li></ul><h3 id=python>Python</h3><ul><li><a href="https://www.youtube.com/watch?v=rfscVS0vtbw">Tutorial</a></li><li>Important to understand:<ul><li>Basics are good - variables + logic, functions, classes</li></ul></li></ul><h3 id=numpy>Numpy</h3><blockquote><p>Must use when working with large arrays (i.e images)</p></blockquote><ul><li><a href="https://www.youtube.com/watch?v=8Y0qQEh7dJg">Tutorial</a></li><li>Important to understand:<ul><li>Creating arrays</li><li>slicing + indexing</li><li>reshaping</li><li>linear algebra</li></ul></li></ul><h2 id=opencv>OpenCV</h2><blockquote><p>Use for computer vision and image transformations like color detection, tracking, etc</p></blockquote><ul><li><a href="https://www.youtube.com/watch?v=oXlwWbU8l2o">Tutorial</a></li><li>Important to understand:<ul><li>image transformation</li><li>thresholding</li><li>read/write images from file</li><li>resizing</li><li>tracking</li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c1fe4798378a51fdf5afb95fe2e6819c>5.2 - Hardware</h1><h2 id=chess-pieces>Chess Pieces</h2><h3 id=design>Design</h3><p>Our current prototype design for the chess pieces has a wooden triangle base, 1" ball bearings, two wheels, and two motors. This was chosen to optimize efficiency, while lowering cost, since we have to build 32 chess pieces. The chess pieces themselves are 9.75" x 7.75" x .5", designed for maneuverability on the chess board, so that they can move between other chess pieces and we don&rsquo;t have to worry about moving other pieces out of the way. We are also using <a href=https://www.pitsco.com/TETRIX-DC-Gear-Motor>TETRIX Motors</a> and <a href=https://www.andymark.com/products/neverest-series-motor-only>Neverest Motors</a>.</p><img src=/wiki/active-projects/wizards-chess/images/cad-v1.png alt="CAD Model of Chess Piece V1" width=600><blockquote><p>Other designs have included an X-drive, 4-wheel drive, or 3-wheel triangle bot, however they were scrapped due to the cost and more complex systems they would have included.</p></blockquote><h3 id=manufacturing>Manufacturing</h3><p>The chess pieces are made from .5" birch plywood and manufactured using the laser cutting process. The hole markers are also included in the laser cutting process so that we don&rsquo;t have to remeasure where to drill the holes after every cut and to keep everything uniform.</p><p>We will also start manufacturing the motor mounts ourselves to cut down on cost, using the manual mill and CNC machine.</p><img src=/wiki/active-projects/wizards-chess/images/laser-cut.png alt="Laser Cut Lines" width=400>
<img src=/wiki/active-projects/wizards-chess/images/motor-mount.jpg alt="CAD Model of Motor Mount" width=400><h2 id=chess-board>Chess Board</h2><h3 id=design-1>Design</h3><p>Each square for the board is 1.5&rsquo; x 1.5&rsquo; to accomodate for the chess pieces moving around and between each other. It is then painted black and white accordingly.</p><blockquote><p>We also considered foam tiles (which would be too expensive) and tape (which is difficult to move).</p></blockquote><h3 id=manufacturing-1>Manufacturing</h3><p>The board is made from 5mm triply and manufactured using the vertical bandsaw. The triply is cut into pieces of 3&rsquo; x 1.5&rsquo; which accomodates 2 squares on the board. Paint and paintbrushes are available in our workspace to paint the board black and white.</p><blockquote><p>When manufacturing, the board is cut to exactly 3&rsquo; and then cut in half, so each piece is a little bit smaller that 1.5&rsquo; due to the saw size.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-f1d1a2f654fdfaef02740bab72b0d54d>5.3 -</h1></div></main></div></div><footer class="bg-dark py-5 row d-print-none"><div class="container-fluid mx-sm-5"><div class=row><div class="col-6 col-sm-4 text-xs-center order-sm-2"></div><div class="col-6 col-sm-4 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank rel=noopener href=https://mobile.twitter.com/purduerobotics aria-label=Twitter><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank rel=noopener href=https://github.com/purdue-arc aria-label=GitHub><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Discord aria-label=Discord><a class=text-white target=_blank rel=noopener href=https://discord.gg/ddkzfD2cyu aria-label=Discord><i class="fab fa-discord"></i></a></li></ul></div><div class="col-12 col-sm-4 text-center py-2 order-sm-2"><small class=text-white>&copy; 2023 Purdue ARC All Rights Reserved</small>
<small class=ml-1><a href=https://policies.google.com/privacy target=_blank rel=noopener>Privacy Policy</a></small></div></div></div></footer></div><script src=/js/main.min.7cefd0b4fd701d2ed63770cd335a32296fb92d97c9b825c65fb3f7853e5a22b9.js integrity="sha256-fO/QtP1wHS7WN3DNM1oyKW+5LZfJuCXGX7P3hT5aIrk=" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script></body></html>